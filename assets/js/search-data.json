{
  
    
        "post0": {
            "title": "Tweet Image Emotion detection - Part 1",
            "content": "About . Prediction using best_checkpoint_CNN1 . Working with pre-trained models: Resnet18, Resnet50 . Beyond the Text . Beside the text, Twitter messages often include images, emojis or other special characters. One approach to understand the overall sentiments from tweets is to work with each layer of information separately and then combining the analysis to reach a result that reflects all layers of the information. This section will look at images separate from the text. Unpacking a way of analyzing images. . How to understand images? . We often understand a complex set of meaning from an image through a quick glance but what does it take for an automated system to understand the interplay of meaning in an image. There are multiple approaches in understanding an image. For instance, an image could be segregating into objects through image segmentation and using further meaning driven analysis a meaning of the image can be formed. But a holistic approach could prove beneficial too. Considering the multitude and complexities that are involved in making sense of images. The first approach here is use a pre-trained resnet model to identify emotions through images. The idea is that we could train a Machine Learning model with a set of images. After the initial training stage, the model should be able to identify the meaning of any new image that we get from tweets for instance. . Project Goals . In this project, I’m looking to identify the overall reaction around a certain topic. One approach would be to identify tweets based on their underlying sentiment, such as anger, joy, sadness, etc. The first step is to find a dataset which includes images with their corresponding labels. This dataset can be used to train the initial model. Looking for such a dataset, I found [Emotion6] dataset from [Cornel University AMP lab] (http://chenlab.ece.cornell.edu/downloads.html). Emotion6 is a collection of 1980 images and is divided into six categories of anger, disgust, fear, joy sadness, neutral . . Citations . [1] Kuan-Chuan Peng, Amir Sadovnik, Andrew Gallagher, and Tsuhan Chen. &quot;A Mixed Bag of Emotions: Model, Predict, and Transfer Emotion Distributions.&quot;, Computer Vision and Pattern Recognition (CVPR), 2015. . import torch import torch.nn as nn from torchvision.transforms import transforms import numpy as np from torch.autograd import Variable from torchvision.models import squeezenet1_1 import torch.functional as F from io import open import os from PIL import Image import pathlib import glob . train_path=&#39;Data/Emotion6/images/emot_train&#39; pred_path=&#39;Data/Emotion6/images/emot_pred&#39; . root=pathlib.Path(train_path) classes=sorted([j.name.split(&#39;/&#39;)[-1] for j in root.iterdir()]) . class ConvNet(nn.Module): def __init__(self,num_classes=6): super(ConvNet,self).__init__() #Output size after convolution filter #((w-f+2P)/s) +1 #Input shape= (256,3,150,150) self.conv1=nn.Conv2d(in_channels=3,out_channels=12,kernel_size=3,stride=1,padding=1) #Shape= (256,12,150,150) self.bn1=nn.BatchNorm2d(num_features=12) #Shape= (256,12,150,150) self.relu1=nn.ReLU() #Shape= (256,12,150,150) self.pool=nn.MaxPool2d(kernel_size=2) #Reduce the image size be factor 2 #Shape= (256,12,75,75) self.conv2=nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1) #Shape= (256,20,75,75) self.relu2=nn.ReLU() #Shape= (256,20,75,75) self.conv3=nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1) #Shape= (256,32,75,75) self.bn3=nn.BatchNorm2d(num_features=32) #Shape= (256,32,75,75) self.relu3=nn.ReLU() #Shape= (256,32,75,75) self.fc=nn.Linear(in_features=75 * 75 * 32,out_features=num_classes) #Feed forwad function def forward(self,input): output=self.conv1(input) output=self.bn1(output) output=self.relu1(output) output=self.pool(output) output=self.conv2(output) output=self.relu2(output) output=self.conv3(output) output=self.bn3(output) output=self.relu3(output) #Above output will be in matrix form, with shape (256,32,75,75) output=output.view(-1,32*75*75) output=self.fc(output) return output . checkpoint=torch.load(&#39;best_checkpoint_CNN1.model&#39;) model=ConvNet(num_classes=6) model.load_state_dict(checkpoint) model.eval() . ConvNet( (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU() (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (conv2): Conv2d(12, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (relu2): ReLU() (conv3): Conv2d(20, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu3): ReLU() (fc): Linear(in_features=180000, out_features=6, bias=True) ) . transformer=transforms.Compose([ transforms.Resize((150,150)), transforms.ToTensor(), #0-255 to 0-1, numpy to tensors transforms.Normalize([0.5,0.5,0.5], # 0-1 to [-1,1] , formula (x-mean)/std [0.5,0.5,0.5]) ]) . def prediction(img_path,transformer): image=Image.open(img_path) image_tensor=transformer(image).float() image_tensor=image_tensor.unsqueeze_(0) if torch.cuda.is_available(): image_tensor.cuda() input=Variable(image_tensor) output=model(input) index=output.data.numpy().argmax() pred=classes[index] return pred . images_path=glob.glob(pred_path+&#39;/*.jpg&#39;) pred_dict={} for i in images_path: pred_dict[i[i.rfind(&#39;/&#39;)+1:]]=prediction(i,transformer) . /home/srds/jupyter_382/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /pytorch/c10/core/TensorImpl.h:1156.) return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode) . pred_dict . {&#39;Image54.jpg&#39;: &#39;surprise&#39;, &#39;Image454.jpg&#39;: &#39;disgust&#39;, &#39;Image5657.jpg&#39;: &#39;disgust&#39;, &#39;ksGhYGkj_bigger.jpg&#39;: &#39;fear&#39;, &#39;sgcP1BRg_bigger.jpg&#39;: &#39;disgust&#39;, &#39;94128202_2868821099834477_4813158761750645393_n.jpg&#39;: &#39;joy&#39;, &#39;Image (11).jpg&#39;: &#39;surprise&#39;, &#39;94103955_161645945308280_8771195847189162660_n.jpg&#39;: &#39;fear&#39;, &#39;Image (1).jpg&#39;: &#39;anger&#39;, &#39;Image56777.jpg&#39;: &#39;disgust&#39;, &#39;Image567.jpg&#39;: &#39;surprise&#39;, &#39;Image (3)676.jpg&#39;: &#39;sadness&#39;, &#39;Image757575.jpg&#39;: &#39;joy&#39;, &#39;Image444.jpg&#39;: &#39;joy&#39;, &#39;Image (10).jpg&#39;: &#39;disgust&#39;, &#39;E6rPXNjWUAIBjJa.jpg&#39;: &#39;fear&#39;, &#39;_4.jpg&#39;: &#39;disgust&#39;, &#39;rHzX97pL_bigger.jpg&#39;: &#39;joy&#39;, &#39;92923611_1318724421851566_555606817225749807_n.jpg&#39;: &#39;anger&#39;, &#39;dcTC4Iqh_bigger.jpg&#39;: &#39;surprise&#39;, &#39;Image5555.jpg&#39;: &#39;disgust&#39;, &#39;Icb_qncl_bigger.jpg&#39;: &#39;anger&#39;, &#39;Image8768.jpg&#39;: &#39;disgust&#39;, &#39;Image5467.jpg&#39;: &#39;disgust&#39;, &#39;_1.jpg&#39;: &#39;sadness&#39;, &#39;Image (13).jpg&#39;: &#39;disgust&#39;, &#39;Image656565.jpg&#39;: &#39;disgust&#39;, &#39;Image (3).jpg&#39;: &#39;joy&#39;, &#39;Image54545.jpg&#39;: &#39;disgust&#39;, &#39;94063514_366264130974642_4624767354939302349_n.jpg&#39;: &#39;joy&#39;, &#39;Image999.jpg&#39;: &#39;joy&#39;, &#39;lfASSkmiIqbwBELK.jpg&#39;: &#39;joy&#39;, &#39;Image23.jpg&#39;: &#39;surprise&#39;, &#39;Image45388.jpg&#39;: &#39;disgust&#39;, &#39;Image887.jpg&#39;: &#39;disgust&#39;, &#39;Image531.jpg&#39;: &#39;joy&#39;, &#39;Image55.jpg&#39;: &#39;surprise&#39;, &#39;Image (1)56756.jpg&#39;: &#39;anger&#39;, &#39;Image.jpg&#39;: &#39;joy&#39;, &#39;Image64.jpg&#39;: &#39;disgust&#39;, &#39;Image743.jpg&#39;: &#39;joy&#39;, &#39;w-TpNJ0OZZh_SLPf.jpg&#39;: &#39;disgust&#39;, &#39;Image (12).jpg&#39;: &#39;joy&#39;, &#39;Image (2).jpg&#39;: &#39;surprise&#39;, &#39;Image (1)5553.jpg&#39;: &#39;disgust&#39;, &#39;Image (1)5.jpg&#39;: &#39;joy&#39;, &#39;Image566.jpg&#39;: &#39;joy&#39;, &#39;E7VO8l3X0AUWw-2.jpg&#39;: &#39;surprise&#39;, &#39;97155132_673810500118160_1507424695014653952_n.jpg&#39;: &#39;disgust&#39;, &#39;Image (15).jpg&#39;: &#39;fear&#39;, &#39;Image5644.jpg&#39;: &#39;surprise&#39;, &#39;Image657.jpg&#39;: &#39;fear&#39;, &#39;Image987.jpg&#39;: &#39;joy&#39;, &#39;Image (3)546.jpg&#39;: &#39;disgust&#39;, &#39;Image3333.jpg&#39;: &#39;anger&#39;, &#39;Image (2)5.jpg&#39;: &#39;anger&#39;, &#39;rCWOzatw_bigger.jpg&#39;: &#39;fear&#39;, &#39;Image (9).jpg&#39;: &#39;surprise&#39;, &#39;Image21.jpg&#39;: &#39;disgust&#39;, &#39;Image56755.jpg&#39;: &#39;disgust&#39;, &#39;Image (1)666.jpg&#39;: &#39;surprise&#39;, &#39;Image4566.jpg&#39;: &#39;disgust&#39;, &#39;Image (5).jpg&#39;: &#39;disgust&#39;, &#39;Image (14).jpg&#39;: &#39;disgust&#39;, &#39;Image 45.jpg&#39;: &#39;disgust&#39;, &#39;Image66.jpg&#39;: &#39;disgust&#39;, &#39;JI-9Xzz9_bigger.jpg&#39;: &#39;disgust&#39;, &#39;Image234.jpg&#39;: &#39;joy&#39;, &#39;Image435667.jpg&#39;: &#39;joy&#39;, &#39;Image (8).jpg&#39;: &#39;disgust&#39;, &#39;Image354.jpg&#39;: &#39;joy&#39;, &#39;Image (3)6786.jpg&#39;: &#39;disgust&#39;, &#39;Image453.jpg&#39;: &#39;surprise&#39;, &#39;P70X22Gj_bigger.jpg&#39;: &#39;anger&#39;, &#39;Image2344.jpg&#39;: &#39;anger&#39;, &#39;Image4576.jpg&#39;: &#39;joy&#39;, &#39;201301956_4484701894887235_1035576262456272086_n.jpg&#39;: &#39;disgust&#39;, &#39;Image (4).jpg&#39;: &#39;disgust&#39;, &#39;Image4444.jpg&#39;: &#39;surprise&#39;, &#39;Image456.jpg&#39;: &#39;joy&#39;, &#39;Image56.jpg&#39;: &#39;anger&#39;, &#39;Image 44.jpg&#39;: &#39;surprise&#39;, &#39;Image (1)32.jpg&#39;: &#39;disgust&#39;, &#39;Image54675.jpg&#39;: &#39;disgust&#39;, &#39;Image (1)7567.jpg&#39;: &#39;disgust&#39;, &#39;Image63.jpg&#39;: &#39;disgust&#39;, &#39;Image (2)546.jpg&#39;: &#39;disgust&#39;, &#39;Image (7).jpg&#39;: &#39;surprise&#39;, &#39;Image (1)456.jpg&#39;: &#39;sadness&#39;, &#39;Image46.jpg&#39;: &#39;joy&#39;, &#39;_2.jpg&#39;: &#39;fear&#39;, &#39;OM4piOux_bigger.jpg&#39;: &#39;disgust&#39;, &#39;E0N7PTlx_bigger.jpg&#39;: &#39;disgust&#39;, &#39;zl6Llik5_bigger.jpg&#39;: &#39;disgust&#39;, &#39;93767281_223116045801198_8078775460886576050_n.jpg&#39;: &#39;disgust&#39;, &#39;Image (2)5345.jpg&#39;: &#39;disgust&#39;, &#39;Image3555.jpg&#39;: &#39;fear&#39;, &#39;Image (6).jpg&#39;: &#39;joy&#39;} . # def imshow(img): # img = img / 2 + 0.5 # unnormalize # plt.imshow(np.transpose(img, (1, 2, 0))) # convert from Tensor image # # obtain one batch of training images # dataiter = iter(train_loader) # images, labels = dataiter.next() # images = images.numpy() # convert images to numpy for display # # plot the images in the batch, along with the corresponding labels # fig = plt.figure(figsize=(10, 4)) # # display 20 images # for idx in np.arange(10): # ax = fig.add_subplot(2, 10/2, idx+1, xticks=[], yticks=[]) # imshow(images[idx]) # ax.set_title(classes[labels[idx]]) . from PIL import Image import matplotlib.pyplot as plt . img_n = 55 img_list = list(pred_dict.keys()) test_image = img_list[img_n] print(f&#39; The predicted label is: {pred_dict[test_image]}&#39;) Image.open(pred_path+&quot;/&quot;+test_image) . The predicted label is: anger . import random import cv2 from matplotlib import pyplot as plt . img_rand_imgs = random.choices(img_list,k=20) . # create figure fig = plt.figure(figsize=(20, 14)) # setting values to rows and column variables rows = 4 columns = 5 i=1 for x in img_rand_imgs: img_path = pred_path + &#39;/&#39;+ x img = cv2.imread(img_path) # Adds a subplot at the 1st position fig.add_subplot(rows, columns, i) # showing image # plt.imshow(img) plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)) plt.axis(&#39;off&#39;) plt.title( f&#39; &lt; {pred_dict[x]} &gt;&#39; ) i = i + 1 .",
            "url": "https://sra00.github.io/notes1/jupyter/2022/06/05/img-1.html",
            "relUrl": "/jupyter/2022/06/05/img-1.html",
            "date": " • Jun 5, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Emotions & Images in Tweets - Part2",
            "content": "Approach . Creating a Custom Convolutional Network . Beyond the Text . Beside the text, Twitter messages often include images, emojis or other special characters. One approach to understand the overall sentiments from tweets is to work with each layer of information separately and then combining the analysis to reach a result that reflects all layers of the information. This section will look at images separate from the text. Unpacking a way of analyzing images. . How to understand images? . We often understand a complex set of meaning from an image through a quick glance but what does it take for an automated system to understand the interplay of meaning in an image. There are multiple approaches in understanding an image. For instance, an image could be segregating into objects through image segmentation and using further meaning driven analysis a meaning of the image can be formed. But a holistic approach could prove beneficial too. Considering the multitude and complexities that are involved in making sense of images. The first approach here is use a pre-trained resnet model to identify emotions through images. The idea is that we could train a Machine Learning model with a set of images. After the initial training stage, the model should be able to identify the meaning of any new image that we get from tweets for instance. . Project Goals . In this project, I’m looking to identify the overall reaction around a certain topic. One approach would be to identify tweets based on their underlying sentiment, such as anger, joy, sadness, etc. The first step is to find a dataset which includes images with their corresponding labels. This dataset can be used to train the initial model. Looking for such a dataset, I found [Emotion6] dataset from [Cornel University AMP lab] (http://chenlab.ece.cornell.edu/downloads.html). Emotion6 is a collection of 1980 images and is divided into six categories of anger, disgust, fear, joy sadness, neutral . . Citations . [1] Kuan-Chuan Peng, Amir Sadovnik, Andrew Gallagher, and Tsuhan Chen. &quot;A Mixed Bag of Emotions: Model, Predict, and Transfer Emotion Distributions.&quot;, Computer Vision and Pattern Recognition (CVPR), 2015. . Step 1- loading data into numpy array &amp; Normalize it divide into test and training datasets . %matplotlib inline . import torch import torchvision import os import numpy as np import glob import torch.nn as nn from torchvision.transforms import transforms from torch.utils.data import DataLoader from torch.optim import Adam from torch.autograd import Variable import torchvision import pathlib import torch.nn.functional as F import matplotlib.pyplot as plt from torchvision import models . . device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) . device . device(type=&#39;cuda&#39;, index=0) . # Image.open(&#39;/Data/Emotions6/Images&#39;).convert(&#39;RGB&#39;) . transformer = transforms.Compose([ transforms.Resize((150,150)), transforms.RandomHorizontalFlip(), transforms.ToTensor(), #0-255 to 0-1, num py to tensors transforms.Normalize([0.5,0.5,0.5], # 0-1 to 1- to 1 [0.5,0.5,0.5]) ]) . test_path = &#39;Data/Emotion6/images/emot_test&#39; train_path = &#39;Data/Emotion6/images/emot_train&#39; batch_size= 10 #DataLoader train_loader = DataLoader( torchvision.datasets.ImageFolder(train_path,transform=transformer), batch_size= 10, shuffle = True ) test_loader = DataLoader( torchvision.datasets.ImageFolder(test_path,transform=transformer), batch_size= 10, shuffle = True ) . dataiter = iter(train_loader) images, labels = dataiter.next() . def imshow(img): img = img / 2 + 0.5 # unnormalize npimg = img.numpy() plt.imshow(np.transpose(npimg, (1, 2, 0))) plt.show() . root = pathlib.Path(train_path) classes = sorted([j.name.split(&#39;/&#39;)[-1] for j in root.iterdir()]) . print(classes) . [&#39;anger&#39;, &#39;disgust&#39;, &#39;fear&#39;, &#39;joy&#39;, &#39;sadness&#39;, &#39;surprise&#39;] . def imshow(img): img = img / 2 + 0.5 # unnormalize plt.imshow(np.transpose(img, (1, 2, 0))) # convert from Tensor image # obtain one batch of training images dataiter = iter(train_loader) images, labels = dataiter.next() images = images.numpy() # convert images to numpy for display # plot the images in the batch, along with the corresponding labels fig = plt.figure(figsize=(10, 4)) # display 10 images for idx in np.arange(10): ax = fig.add_subplot(2, 10/2, idx+1, xticks=[], yticks=[]) imshow(images[idx]) ax.set_title(classes[labels[idx]]) . /tmp/ipykernel_3496/1155777525.py:15: MatplotlibDeprecationWarning: Passing non-integers as three-element position specification is deprecated since 3.3 and will be removed two minor releases later. ax = fig.add_subplot(2, 10/2, idx+1, xticks=[], yticks=[]) . class ConvNet(nn.Module): def __init__(self,num_classes=6): super(ConvNet,self).__init__() #Output size after convolution filter #((w-f+2P)/s) +1 #Input shape= (256,3,150,150) self.conv1=nn.Conv2d(in_channels=3,out_channels=12,kernel_size=3,stride=1,padding=1) self.bn1=nn.BatchNorm2d(num_features=12) self.relu1=nn.ReLU() self.pool=nn.MaxPool2d(kernel_size=2) self.conv2=nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1) self.relu2=nn.ReLU() self.conv3=nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1) self.bn3=nn.BatchNorm2d(num_features=32) self.relu3=nn.ReLU() # self.conv4=nn.Conv2d(in_channels=32,out_channels=46,kernel_size=3,stride=1,padding=1) # self.bn4=nn.BatchNorm2d(num_features=46) # self.relu4=nn.ReLU() self.fc=nn.Linear(in_features=75 * 75 * 32 ,out_features=num_classes) #Feed forwad function def forward(self,input): output=self.conv1(input) output=self.bn1(output) output=self.relu1(output) output=self.pool(output) output=self.conv2(output) output=self.relu2(output) output=self.conv3(output) output=self.bn3(output) output=self.relu3(output) #Above output will be in matrix form, with shape (256,32,75,75) output=output.view(-1,32*75*75) output=self.fc(output) return output . model = ConvNet(num_classes=6).to(device) # model = Net() # model = models.vgg16(pretrained=True) # model = models.vgg16(pretrained=True) . optimizer=Adam(model.parameters(),lr=0.001,weight_decay=0.0001) loss_function=nn.CrossEntropyLoss() . train_count=len(glob.glob(train_path+&#39;/**/*.jpg&#39;)) test_count=len(glob.glob(test_path+&#39;/**/*.jpg&#39;)) . print(train_count,test_count) . 1584 396 . num_epochs=11 . best_accuracy=0.0 for epoch in range(num_epochs): #1-Evaluation and training on training dataset model.train() train_accuracy=0.0 train_loss=0.0 #training loader loop for i, (images,labels) in enumerate(train_loader): if torch.cuda.is_available(): images=Variable(images.cuda()) labels=Variable(labels.cuda()) optimizer.zero_grad() outputs=model(images) loss=loss_function(outputs,labels) loss.backward() optimizer.step() train_loss+= loss.cpu().data*images.size(0) _,prediction=torch.max(outputs.data,1) train_accuracy+=int(torch.sum(prediction==labels.data)) train_accuracy=train_accuracy/train_count train_loss=train_loss/train_count #2-Evaluation on testing dataset model.eval() test_accuracy=0.0 for i, (images,labels) in enumerate(test_loader): if torch.cuda.is_available(): images=Variable(images.cuda()) labels=Variable(labels.cuda()) outputs=model(images) _,prediction=torch.max(outputs.data,1) test_accuracy+=int(torch.sum(prediction==labels.data)) test_accuracy=test_accuracy/test_count print(&#39;Epoch: &#39;+str(epoch)+&#39; Train Loss: &#39;+str(train_loss)+&#39; Train Accuracy: &#39;+str(train_accuracy)+&#39; Test Accuracy: &#39;+str(test_accuracy)) #Save the best model if test_accuracy&gt;best_accuracy: torch.save(model.state_dict(),&#39;best_checkpoint_CNN1.model&#39;) best_accuracy=test_accuracy . Epoch: 0 Train Loss: tensor(17.3005) Train Accuracy: 0.20012626262626262 Test Accuracy: 0.23484848484848486 Epoch: 1 Train Loss: tensor(5.1706) Train Accuracy: 0.39330808080808083 Test Accuracy: 0.20707070707070707 Epoch: 2 Train Loss: tensor(1.8187) Train Accuracy: 0.5271464646464646 Test Accuracy: 0.22474747474747475 Epoch: 3 Train Loss: tensor(0.9630) Train Accuracy: 0.6830808080808081 Test Accuracy: 0.2601010101010101 Epoch: 4 Train Loss: tensor(0.5627) Train Accuracy: 0.8244949494949495 Test Accuracy: 0.2196969696969697 Epoch: 5 Train Loss: tensor(0.4051) Train Accuracy: 0.8819444444444444 Test Accuracy: 0.20959595959595959 Epoch: 6 Train Loss: tensor(0.3027) Train Accuracy: 0.9084595959595959 Test Accuracy: 0.2297979797979798 Epoch: 7 Train Loss: tensor(0.2230) Train Accuracy: 0.9311868686868687 Test Accuracy: 0.255050505050505 Epoch: 8 Train Loss: tensor(0.1864) Train Accuracy: 0.9457070707070707 Test Accuracy: 0.24494949494949494 Epoch: 9 Train Loss: tensor(0.1427) Train Accuracy: 0.9595959595959596 Test Accuracy: 0.22474747474747475 Epoch: 10 Train Loss: tensor(0.1100) Train Accuracy: 0.9678030303030303 Test Accuracy: 0.24494949494949494 .",
            "url": "https://sra00.github.io/notes1/jupyter/2022/06/04/image-2.html",
            "relUrl": "/jupyter/2022/06/04/image-2.html",
            "date": " • Jun 4, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Tweet Image Emotion detection - Part 3",
            "content": "About . Prediction using best_checkpoint_CNN1 . Working with pre-trained models: Resnet18, Resnet50 . Beyond the Text . Beside the text, Twitter messages often include images, emojis or other special characters. One approach to understand the overall sentiments from tweets is to work with each layer of information separately and then combining the analysis to reach a result that reflects all layers of the information. This section will look at images separate from the text. Unpacking a way of analyzing images. . How to understand images? . We often understand a complex set of meaning from an image through a quick glance but what does it take for an automated system to understand the interplay of meaning in an image. There are multiple approaches in understanding an image. For instance, an image could be segregating into objects through image segmentation and using further meaning driven analysis a meaning of the image can be formed. But a holistic approach could prove beneficial too. Considering the multitude and complexities that are involved in making sense of images. The first approach here is use a pre-trained resnet model to identify emotions through images. The idea is that we could train a Machine Learning model with a set of images. After the initial training stage, the model should be able to identify the meaning of any new image that we get from tweets for instance. . Project Goals . In this project, I’m looking to identify the overall reaction around a certain topic. One approach would be to identify tweets based on their underlying sentiment, such as anger, joy, sadness, etc. The first step is to find a dataset which includes images with their corresponding labels. This dataset can be used to train the initial model. Looking for such a dataset, I found [Emotion6] dataset from [Cornel University AMP lab] (http://chenlab.ece.cornell.edu/downloads.html). Emotion6 is a collection of 1980 images and is divided into six categories of anger, disgust, fear, joy sadness, neutral . . Citations . [1] Kuan-Chuan Peng, Amir Sadovnik, Andrew Gallagher, and Tsuhan Chen. &quot;A Mixed Bag of Emotions: Model, Predict, and Transfer Emotion Distributions.&quot;, Computer Vision and Pattern Recognition (CVPR), 2015. . Training the model . import torch import torch.nn as nn from torchvision.transforms import transforms import numpy as np from torch.autograd import Variable from torchvision.models import squeezenet1_1 import torch.functional as F from io import open import os from PIL import Image import pathlib import glob from torchvision import models . train_path=&#39;Data/Emotion6/images/emot_train&#39; pred_path=&#39;Data/Emotion6/images/emot_pred&#39; . root=pathlib.Path(train_path) classes=sorted([j.name.split(&#39;/&#39;)[-1] for j in root.iterdir()]) . class ConvNet(nn.Module): def __init__(self,num_classes=6): super(ConvNet,self).__init__() #Output size after convolution filter #((w-f+2P)/s) +1 #Input shape= (256,3,150,150) self.conv1=nn.Conv2d(in_channels=3,out_channels=12,kernel_size=3,stride=1,padding=1) #Shape= (256,12,150,150) self.bn1=nn.BatchNorm2d(num_features=12) #Shape= (256,12,150,150) self.relu1=nn.ReLU() #Shape= (256,12,150,150) self.pool=nn.MaxPool2d(kernel_size=2) #Reduce the image size be factor 2 #Shape= (256,12,75,75) self.conv2=nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1) #Shape= (256,20,75,75) self.relu2=nn.ReLU() #Shape= (256,20,75,75) self.conv3=nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1) #Shape= (256,32,75,75) self.bn3=nn.BatchNorm2d(num_features=32) #Shape= (256,32,75,75) self.relu3=nn.ReLU() #Shape= (256,32,75,75) self.fc=nn.Linear(in_features=75 * 75 * 32,out_features=num_classes) #Feed forwad function def forward(self,input): output=self.conv1(input) output=self.bn1(output) output=self.relu1(output) output=self.pool(output) output=self.conv2(output) output=self.relu2(output) output=self.conv3(output) output=self.bn3(output) output=self.relu3(output) #Above output will be in matrix form, with shape (256,32,75,75) output=output.view(-1,32*75*75) output=self.fc(output) return output . class AlexNet(nn.Module): def __init__(self, num_classes): super(AlexNet, self).__init__() self.features = nn.Sequential( nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(64, 192, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(192, 384, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), ) self.avgpool = nn.AdaptiveAvgPool2d((6, 6)) self.classifier = nn.Sequential( nn.Dropout(0.5), nn.Linear(256 * 6 * 6, 4096), nn.ReLU(inplace=True), nn.Dropout(0.5), nn.Linear(4096, 4096), nn.ReLU(inplace=True), nn.Linear(4096, num_classes) ) def forward(self, x): x = self.features(x) x = self.avgpool(x) x = x.view(x.size(0), 256 * 6 * 6) logits = self.classifier(x) # probas = F.softmax(logits, dim=1) return logits . checkpoint=torch.load(&#39;best_checkpoint.model&#39;) # model = models.vgg16(pretrained=False) # model=ConvNet(num_classes=6) # model=AlexNet(num_classes=6) model = models.resnet18() model.load_state_dict(checkpoint) model.eval() . ResNet( (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (layer1): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer2): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer3): Sequential( (0): BasicBlock( (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer4): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (avgpool): AdaptiveAvgPool2d(output_size=(1, 1)) (fc): Linear(in_features=512, out_features=1000, bias=True) ) . transformer=transforms.Compose([ transforms.Resize((150,150)), transforms.ToTensor(), #0-255 to 0-1, numpy to tensors transforms.Normalize([0.5,0.5,0.5], # 0-1 to [-1,1] , formula (x-mean)/std [0.5,0.5,0.5]) ]) images_path=glob.glob(pred_path+&#39;/*.jpg&#39;) . def prediction(img_path,transformer): image=Image.open(img_path) image_tensor=transformer(image).float() image_tensor=image_tensor.unsqueeze_(0) if torch.cuda.is_available(): image_tensor.cuda() input=Variable(image_tensor) output=model(input) index=output.data.numpy().argmax() pred=classes[index] return pred . pred_dict={} for i in images_path: pred_dict[i[i.rfind(&#39;/&#39;)+1:]]=prediction(i,transformer) . pred_dict . {&#39;Image54.jpg&#39;: &#39;joy&#39;, &#39;Image454.jpg&#39;: &#39;joy&#39;, &#39;Image5657.jpg&#39;: &#39;joy&#39;, &#39;ksGhYGkj_bigger.jpg&#39;: &#39;joy&#39;, &#39;sgcP1BRg_bigger.jpg&#39;: &#39;sadness&#39;, &#39;94128202_2868821099834477_4813158761750645393_n.jpg&#39;: &#39;sadness&#39;, &#39;Image (11).jpg&#39;: &#39;joy&#39;, &#39;94103955_161645945308280_8771195847189162660_n.jpg&#39;: &#39;sadness&#39;, &#39;Image (1).jpg&#39;: &#39;surprise&#39;, &#39;Image56777.jpg&#39;: &#39;joy&#39;, &#39;Image567.jpg&#39;: &#39;surprise&#39;, &#39;Image (3)676.jpg&#39;: &#39;joy&#39;, &#39;Image757575.jpg&#39;: &#39;joy&#39;, &#39;Image444.jpg&#39;: &#39;joy&#39;, &#39;Image (10).jpg&#39;: &#39;disgust&#39;, &#39;E6rPXNjWUAIBjJa.jpg&#39;: &#39;disgust&#39;, &#39;_4.jpg&#39;: &#39;disgust&#39;, &#39;rHzX97pL_bigger.jpg&#39;: &#39;joy&#39;, &#39;92923611_1318724421851566_555606817225749807_n.jpg&#39;: &#39;joy&#39;, &#39;dcTC4Iqh_bigger.jpg&#39;: &#39;joy&#39;, &#39;Image5555.jpg&#39;: &#39;disgust&#39;, &#39;Icb_qncl_bigger.jpg&#39;: &#39;joy&#39;, &#39;Image8768.jpg&#39;: &#39;fear&#39;, &#39;Image5467.jpg&#39;: &#39;joy&#39;, &#39;_1.jpg&#39;: &#39;joy&#39;, &#39;Image (13).jpg&#39;: &#39;disgust&#39;, &#39;Image656565.jpg&#39;: &#39;disgust&#39;, &#39;Image (3).jpg&#39;: &#39;fear&#39;, &#39;Image54545.jpg&#39;: &#39;disgust&#39;, &#39;94063514_366264130974642_4624767354939302349_n.jpg&#39;: &#39;sadness&#39;, &#39;Image999.jpg&#39;: &#39;joy&#39;, &#39;lfASSkmiIqbwBELK.jpg&#39;: &#39;joy&#39;, &#39;Image23.jpg&#39;: &#39;fear&#39;, &#39;Image45388.jpg&#39;: &#39;sadness&#39;, &#39;Image887.jpg&#39;: &#39;sadness&#39;, &#39;Image531.jpg&#39;: &#39;joy&#39;, &#39;Image55.jpg&#39;: &#39;joy&#39;, &#39;Image (1)56756.jpg&#39;: &#39;joy&#39;, &#39;Image.jpg&#39;: &#39;fear&#39;, &#39;Image64.jpg&#39;: &#39;disgust&#39;, &#39;Image743.jpg&#39;: &#39;joy&#39;, &#39;w-TpNJ0OZZh_SLPf.jpg&#39;: &#39;joy&#39;, &#39;Image (12).jpg&#39;: &#39;fear&#39;, &#39;Image (2).jpg&#39;: &#39;fear&#39;, &#39;Image (1)5553.jpg&#39;: &#39;sadness&#39;, &#39;Image (1)5.jpg&#39;: &#39;disgust&#39;, &#39;Image566.jpg&#39;: &#39;joy&#39;, &#39;E7VO8l3X0AUWw-2.jpg&#39;: &#39;disgust&#39;, &#39;97155132_673810500118160_1507424695014653952_n.jpg&#39;: &#39;sadness&#39;, &#39;Image (15).jpg&#39;: &#39;disgust&#39;, &#39;Image5644.jpg&#39;: &#39;fear&#39;, &#39;Image657.jpg&#39;: &#39;sadness&#39;, &#39;Image987.jpg&#39;: &#39;joy&#39;, &#39;Image (3)546.jpg&#39;: &#39;joy&#39;, &#39;Image3333.jpg&#39;: &#39;sadness&#39;, &#39;Image (2)5.jpg&#39;: &#39;fear&#39;, &#39;rCWOzatw_bigger.jpg&#39;: &#39;fear&#39;, &#39;Image (9).jpg&#39;: &#39;surprise&#39;, &#39;Image21.jpg&#39;: &#39;joy&#39;, &#39;Image56755.jpg&#39;: &#39;fear&#39;, &#39;Image (1)666.jpg&#39;: &#39;joy&#39;, &#39;Image4566.jpg&#39;: &#39;joy&#39;, &#39;Image (5).jpg&#39;: &#39;disgust&#39;, &#39;Image (14).jpg&#39;: &#39;disgust&#39;, &#39;Image 45.jpg&#39;: &#39;joy&#39;, &#39;Image66.jpg&#39;: &#39;joy&#39;, &#39;JI-9Xzz9_bigger.jpg&#39;: &#39;joy&#39;, &#39;Image234.jpg&#39;: &#39;fear&#39;, &#39;Image435667.jpg&#39;: &#39;joy&#39;, &#39;Image (8).jpg&#39;: &#39;disgust&#39;, &#39;Image354.jpg&#39;: &#39;disgust&#39;, &#39;Image (3)6786.jpg&#39;: &#39;joy&#39;, &#39;Image453.jpg&#39;: &#39;fear&#39;, &#39;P70X22Gj_bigger.jpg&#39;: &#39;joy&#39;, &#39;Image2344.jpg&#39;: &#39;sadness&#39;, &#39;Image4576.jpg&#39;: &#39;joy&#39;, &#39;201301956_4484701894887235_1035576262456272086_n.jpg&#39;: &#39;fear&#39;, &#39;Image (4).jpg&#39;: &#39;fear&#39;, &#39;Image4444.jpg&#39;: &#39;joy&#39;, &#39;Image456.jpg&#39;: &#39;surprise&#39;, &#39;Image56.jpg&#39;: &#39;surprise&#39;, &#39;Image 44.jpg&#39;: &#39;sadness&#39;, &#39;Image (1)32.jpg&#39;: &#39;fear&#39;, &#39;Image54675.jpg&#39;: &#39;joy&#39;, &#39;Image (1)7567.jpg&#39;: &#39;disgust&#39;, &#39;Image63.jpg&#39;: &#39;surprise&#39;, &#39;Image (2)546.jpg&#39;: &#39;joy&#39;, &#39;Image (7).jpg&#39;: &#39;disgust&#39;, &#39;Image (1)456.jpg&#39;: &#39;joy&#39;, &#39;Image46.jpg&#39;: &#39;joy&#39;, &#39;_2.jpg&#39;: &#39;sadness&#39;, &#39;OM4piOux_bigger.jpg&#39;: &#39;joy&#39;, &#39;E0N7PTlx_bigger.jpg&#39;: &#39;joy&#39;, &#39;zl6Llik5_bigger.jpg&#39;: &#39;joy&#39;, &#39;93767281_223116045801198_8078775460886576050_n.jpg&#39;: &#39;fear&#39;, &#39;Image (2)5345.jpg&#39;: &#39;disgust&#39;, &#39;Image3555.jpg&#39;: &#39;joy&#39;, &#39;Image (6).jpg&#39;: &#39;surprise&#39;} . # def imshow(img): # img = img / 2 + 0.5 # unnormalize # plt.imshow(np.transpose(img, (1, 2, 0))) # convert from Tensor image # # obtain one batch of training images # dataiter = iter(train_loader) # images, labels = dataiter.next() # images = images.numpy() # convert images to numpy for display # # plot the images in the batch, along with the corresponding labels # fig = plt.figure(figsize=(10, 4)) # # display 20 images # for idx in np.arange(10): # ax = fig.add_subplot(2, 10/2, idx+1, xticks=[], yticks=[]) # imshow(images[idx]) # ax.set_title(classes[labels[idx]]) . from PIL import Image import matplotlib.pyplot as plt . img_n = 50 img_list = list(pred_dict.keys()) test_image = img_list[img_n] print(f&#39; The predicted label is: {pred_dict[test_image]}&#39;) Image.open(pred_path+&quot;/&quot;+test_image) . The predicted label is: fear . import random import cv2 from matplotlib import pyplot as plt . img_rand_imgs = random.choices(img_list,k=20) . # create figure fig = plt.figure(figsize=(20, 14)) # setting values to rows and column variables rows = 4 columns = 5 i=1 for x in img_rand_imgs: img_path = pred_path + &#39;/&#39;+ x img = cv2.imread(img_path) # Adds a subplot at the 1st position fig.add_subplot(rows, columns, i) # showing image # plt.imshow(img) plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)) plt.axis(&#39;off&#39;) plt.title( f&#39; &lt; {pred_dict[x]} &gt;&#39; ) i = i + 1 . type(pred_dict) . dict . import pandas as pd . pd_img_results = pd.DataFrame([pred_dict]) . df = pd.DataFrame(list(pred_dict.items()),columns = [&#39;filename&#39;,&#39;emotion&#39;]) grouped = df.groupby(&quot;emotion&quot;) . grouped.count() . filename . emotion . disgust 13 | . fear 5 | . joy 35 | . sadness 10 | . surprise 35 | .",
            "url": "https://sra00.github.io/notes1/jupyter/2021/11/08/pred2-Image-3.html",
            "relUrl": "/jupyter/2021/11/08/pred2-Image-3.html",
            "date": " • Nov 8, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://sra00.github.io/notes1/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is to showcase Twitter Analysis Project 1. . Includes Jupyter notebooks in addition to other files. &#8617; . |",
          "url": "https://sra00.github.io/notes1/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://sra00.github.io/notes1/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}