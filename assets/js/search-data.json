{
  
    
        "post0": {
            "title": "Tweet Image Emotion detection - Part 1",
            "content": "About . Prediction using best_checkpoint_CNN1 . Working with pre-trained models: Resnet18, Resnet50 . Beyond the Text . Beside the text, Twitter messages often include images, emojis or other special characters. One approach to understand the overall sentiments from tweets is to work with each layer of information separately and then combining the analysis to reach a result that reflects all layers of the information. This section will look at images separate from the text. Unpacking a way of analyzing images. . How to understand images? . We often understand a complex set of meaning from an image through a quick glance but what does it take for an automated system to understand the interplay of meaning in an image. There are multiple approaches in understanding an image. For instance, an image could be segregating into objects through image segmentation and using further meaning driven analysis a meaning of the image can be formed. But a holistic approach could prove beneficial too. Considering the multitude and complexities that are involved in making sense of images. The first approach here is use a pre-trained resnet model to identify emotions through images. The idea is that we could train a Machine Learning model with a set of images. After the initial training stage, the model should be able to identify the meaning of any new image that we get from tweets for instance. . Project Goals . In this project, I’m looking to identify the overall reaction around a certain topic. One approach would be to identify tweets based on their underlying sentiment, such as anger, joy, sadness, etc. The first step is to find a dataset which includes images with their corresponding labels. This dataset can be used to train the initial model. Looking for such a dataset, I found [Emotion6] dataset from [Cornel University AMP lab] (http://chenlab.ece.cornell.edu/downloads.html). Emotion6 is a collection of 1980 images and is divided into six categories of anger, disgust, fear, joy sadness, neutral . . Citations . [1] Kuan-Chuan Peng, Amir Sadovnik, Andrew Gallagher, and Tsuhan Chen. &quot;A Mixed Bag of Emotions: Model, Predict, and Transfer Emotion Distributions.&quot;, Computer Vision and Pattern Recognition (CVPR), 2015. . import torch import torch.nn as nn from torchvision.transforms import transforms import numpy as np from torch.autograd import Variable from torchvision.models import squeezenet1_1 import torch.functional as F from io import open import os from PIL import Image import pathlib import glob . train_path=&#39;Data/Emotion6/images/emot_train&#39; pred_path=&#39;Data/Emotion6/images/emot_pred&#39; . root=pathlib.Path(train_path) classes=sorted([j.name.split(&#39;/&#39;)[-1] for j in root.iterdir()]) . class ConvNet(nn.Module): def __init__(self,num_classes=6): super(ConvNet,self).__init__() #Output size after convolution filter #((w-f+2P)/s) +1 #Input shape= (256,3,150,150) self.conv1=nn.Conv2d(in_channels=3,out_channels=12,kernel_size=3,stride=1,padding=1) #Shape= (256,12,150,150) self.bn1=nn.BatchNorm2d(num_features=12) #Shape= (256,12,150,150) self.relu1=nn.ReLU() #Shape= (256,12,150,150) self.pool=nn.MaxPool2d(kernel_size=2) #Reduce the image size be factor 2 #Shape= (256,12,75,75) self.conv2=nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1) #Shape= (256,20,75,75) self.relu2=nn.ReLU() #Shape= (256,20,75,75) self.conv3=nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1) #Shape= (256,32,75,75) self.bn3=nn.BatchNorm2d(num_features=32) #Shape= (256,32,75,75) self.relu3=nn.ReLU() #Shape= (256,32,75,75) self.fc=nn.Linear(in_features=75 * 75 * 32,out_features=num_classes) #Feed forwad function def forward(self,input): output=self.conv1(input) output=self.bn1(output) output=self.relu1(output) output=self.pool(output) output=self.conv2(output) output=self.relu2(output) output=self.conv3(output) output=self.bn3(output) output=self.relu3(output) #Above output will be in matrix form, with shape (256,32,75,75) output=output.view(-1,32*75*75) output=self.fc(output) return output . checkpoint=torch.load(&#39;best_checkpoint_CNN1.model&#39;) model=ConvNet(num_classes=6) model.load_state_dict(checkpoint) model.eval() . ConvNet( (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU() (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (conv2): Conv2d(12, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (relu2): ReLU() (conv3): Conv2d(20, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu3): ReLU() (fc): Linear(in_features=180000, out_features=6, bias=True) ) . transformer=transforms.Compose([ transforms.Resize((150,150)), transforms.ToTensor(), #0-255 to 0-1, numpy to tensors transforms.Normalize([0.5,0.5,0.5], # 0-1 to [-1,1] , formula (x-mean)/std [0.5,0.5,0.5]) ]) . def prediction(img_path,transformer): image=Image.open(img_path) image_tensor=transformer(image).float() image_tensor=image_tensor.unsqueeze_(0) if torch.cuda.is_available(): image_tensor.cuda() input=Variable(image_tensor) output=model(input) index=output.data.numpy().argmax() pred=classes[index] return pred . images_path=glob.glob(pred_path+&#39;/*.jpg&#39;) pred_dict={} for i in images_path: pred_dict[i[i.rfind(&#39;/&#39;)+1:]]=prediction(i,transformer) . /home/srds/jupyter_382/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /pytorch/c10/core/TensorImpl.h:1156.) return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode) . pred_dict . {&#39;Image54.jpg&#39;: &#39;surprise&#39;, &#39;Image454.jpg&#39;: &#39;disgust&#39;, &#39;Image5657.jpg&#39;: &#39;disgust&#39;, &#39;ksGhYGkj_bigger.jpg&#39;: &#39;fear&#39;, &#39;sgcP1BRg_bigger.jpg&#39;: &#39;disgust&#39;, &#39;94128202_2868821099834477_4813158761750645393_n.jpg&#39;: &#39;joy&#39;, &#39;Image (11).jpg&#39;: &#39;surprise&#39;, &#39;94103955_161645945308280_8771195847189162660_n.jpg&#39;: &#39;fear&#39;, &#39;Image (1).jpg&#39;: &#39;anger&#39;, &#39;Image56777.jpg&#39;: &#39;disgust&#39;, &#39;Image567.jpg&#39;: &#39;surprise&#39;, &#39;Image (3)676.jpg&#39;: &#39;sadness&#39;, &#39;Image757575.jpg&#39;: &#39;joy&#39;, &#39;Image444.jpg&#39;: &#39;joy&#39;, &#39;Image (10).jpg&#39;: &#39;disgust&#39;, &#39;E6rPXNjWUAIBjJa.jpg&#39;: &#39;fear&#39;, &#39;_4.jpg&#39;: &#39;disgust&#39;, &#39;rHzX97pL_bigger.jpg&#39;: &#39;joy&#39;, &#39;92923611_1318724421851566_555606817225749807_n.jpg&#39;: &#39;anger&#39;, &#39;dcTC4Iqh_bigger.jpg&#39;: &#39;surprise&#39;, &#39;Image5555.jpg&#39;: &#39;disgust&#39;, &#39;Icb_qncl_bigger.jpg&#39;: &#39;anger&#39;, &#39;Image8768.jpg&#39;: &#39;disgust&#39;, &#39;Image5467.jpg&#39;: &#39;disgust&#39;, &#39;_1.jpg&#39;: &#39;sadness&#39;, &#39;Image (13).jpg&#39;: &#39;disgust&#39;, &#39;Image656565.jpg&#39;: &#39;disgust&#39;, &#39;Image (3).jpg&#39;: &#39;joy&#39;, &#39;Image54545.jpg&#39;: &#39;disgust&#39;, &#39;94063514_366264130974642_4624767354939302349_n.jpg&#39;: &#39;joy&#39;, &#39;Image999.jpg&#39;: &#39;joy&#39;, &#39;lfASSkmiIqbwBELK.jpg&#39;: &#39;joy&#39;, &#39;Image23.jpg&#39;: &#39;surprise&#39;, &#39;Image45388.jpg&#39;: &#39;disgust&#39;, &#39;Image887.jpg&#39;: &#39;disgust&#39;, &#39;Image531.jpg&#39;: &#39;joy&#39;, &#39;Image55.jpg&#39;: &#39;surprise&#39;, &#39;Image (1)56756.jpg&#39;: &#39;anger&#39;, &#39;Image.jpg&#39;: &#39;joy&#39;, &#39;Image64.jpg&#39;: &#39;disgust&#39;, &#39;Image743.jpg&#39;: &#39;joy&#39;, &#39;w-TpNJ0OZZh_SLPf.jpg&#39;: &#39;disgust&#39;, &#39;Image (12).jpg&#39;: &#39;joy&#39;, &#39;Image (2).jpg&#39;: &#39;surprise&#39;, &#39;Image (1)5553.jpg&#39;: &#39;disgust&#39;, &#39;Image (1)5.jpg&#39;: &#39;joy&#39;, &#39;Image566.jpg&#39;: &#39;joy&#39;, &#39;E7VO8l3X0AUWw-2.jpg&#39;: &#39;surprise&#39;, &#39;97155132_673810500118160_1507424695014653952_n.jpg&#39;: &#39;disgust&#39;, &#39;Image (15).jpg&#39;: &#39;fear&#39;, &#39;Image5644.jpg&#39;: &#39;surprise&#39;, &#39;Image657.jpg&#39;: &#39;fear&#39;, &#39;Image987.jpg&#39;: &#39;joy&#39;, &#39;Image (3)546.jpg&#39;: &#39;disgust&#39;, &#39;Image3333.jpg&#39;: &#39;anger&#39;, &#39;Image (2)5.jpg&#39;: &#39;anger&#39;, &#39;rCWOzatw_bigger.jpg&#39;: &#39;fear&#39;, &#39;Image (9).jpg&#39;: &#39;surprise&#39;, &#39;Image21.jpg&#39;: &#39;disgust&#39;, &#39;Image56755.jpg&#39;: &#39;disgust&#39;, &#39;Image (1)666.jpg&#39;: &#39;surprise&#39;, &#39;Image4566.jpg&#39;: &#39;disgust&#39;, &#39;Image (5).jpg&#39;: &#39;disgust&#39;, &#39;Image (14).jpg&#39;: &#39;disgust&#39;, &#39;Image 45.jpg&#39;: &#39;disgust&#39;, &#39;Image66.jpg&#39;: &#39;disgust&#39;, &#39;JI-9Xzz9_bigger.jpg&#39;: &#39;disgust&#39;, &#39;Image234.jpg&#39;: &#39;joy&#39;, &#39;Image435667.jpg&#39;: &#39;joy&#39;, &#39;Image (8).jpg&#39;: &#39;disgust&#39;, &#39;Image354.jpg&#39;: &#39;joy&#39;, &#39;Image (3)6786.jpg&#39;: &#39;disgust&#39;, &#39;Image453.jpg&#39;: &#39;surprise&#39;, &#39;P70X22Gj_bigger.jpg&#39;: &#39;anger&#39;, &#39;Image2344.jpg&#39;: &#39;anger&#39;, &#39;Image4576.jpg&#39;: &#39;joy&#39;, &#39;201301956_4484701894887235_1035576262456272086_n.jpg&#39;: &#39;disgust&#39;, &#39;Image (4).jpg&#39;: &#39;disgust&#39;, &#39;Image4444.jpg&#39;: &#39;surprise&#39;, &#39;Image456.jpg&#39;: &#39;joy&#39;, &#39;Image56.jpg&#39;: &#39;anger&#39;, &#39;Image 44.jpg&#39;: &#39;surprise&#39;, &#39;Image (1)32.jpg&#39;: &#39;disgust&#39;, &#39;Image54675.jpg&#39;: &#39;disgust&#39;, &#39;Image (1)7567.jpg&#39;: &#39;disgust&#39;, &#39;Image63.jpg&#39;: &#39;disgust&#39;, &#39;Image (2)546.jpg&#39;: &#39;disgust&#39;, &#39;Image (7).jpg&#39;: &#39;surprise&#39;, &#39;Image (1)456.jpg&#39;: &#39;sadness&#39;, &#39;Image46.jpg&#39;: &#39;joy&#39;, &#39;_2.jpg&#39;: &#39;fear&#39;, &#39;OM4piOux_bigger.jpg&#39;: &#39;disgust&#39;, &#39;E0N7PTlx_bigger.jpg&#39;: &#39;disgust&#39;, &#39;zl6Llik5_bigger.jpg&#39;: &#39;disgust&#39;, &#39;93767281_223116045801198_8078775460886576050_n.jpg&#39;: &#39;disgust&#39;, &#39;Image (2)5345.jpg&#39;: &#39;disgust&#39;, &#39;Image3555.jpg&#39;: &#39;fear&#39;, &#39;Image (6).jpg&#39;: &#39;joy&#39;} . # def imshow(img): # img = img / 2 + 0.5 # unnormalize # plt.imshow(np.transpose(img, (1, 2, 0))) # convert from Tensor image # # obtain one batch of training images # dataiter = iter(train_loader) # images, labels = dataiter.next() # images = images.numpy() # convert images to numpy for display # # plot the images in the batch, along with the corresponding labels # fig = plt.figure(figsize=(10, 4)) # # display 20 images # for idx in np.arange(10): # ax = fig.add_subplot(2, 10/2, idx+1, xticks=[], yticks=[]) # imshow(images[idx]) # ax.set_title(classes[labels[idx]]) . from PIL import Image import matplotlib.pyplot as plt . img_n = 55 img_list = list(pred_dict.keys()) test_image = img_list[img_n] print(f&#39; The predicted label is: {pred_dict[test_image]}&#39;) Image.open(pred_path+&quot;/&quot;+test_image) . The predicted label is: anger . import random import cv2 from matplotlib import pyplot as plt . img_rand_imgs = random.choices(img_list,k=20) . # create figure fig = plt.figure(figsize=(20, 14)) # setting values to rows and column variables rows = 4 columns = 5 i=1 for x in img_rand_imgs: img_path = pred_path + &#39;/&#39;+ x img = cv2.imread(img_path) # Adds a subplot at the 1st position fig.add_subplot(rows, columns, i) # showing image # plt.imshow(img) plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)) plt.axis(&#39;off&#39;) plt.title( f&#39; &lt; {pred_dict[x]} &gt;&#39; ) i = i + 1 .",
            "url": "https://sra00.github.io/notes1/jupyter/2022/06/05/img-1.html",
            "relUrl": "/jupyter/2022/06/05/img-1.html",
            "date": " • Jun 5, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Tweets Analysis",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Add the libraries and the data set . import pandas as pd import numpy as np import re from textblob import TextBlob from wordcloud import WordCloud import matplotlib.pyplot as plt import seaborn as sns import string import nltk import warnings %matplotlib inline ## Install this on a new system # nltk.download(&#39;punkt&#39;) from nltk.tokenize import word_tokenize warnings.filterwarnings(&#39;ignore&#39;) plt.style.use(&#39;fivethirtyeight&#39;) . Step 1: Reading the dataset . In this step we read the .csv file that we downloaded from the Hydrator The .csv file contains all the column and includes various meta deta, we need to identify which columns are needed here. Since, I&#39;ll be only working with the Text column in the data. If we need to include hashtags, we add it to the columns as well . Get the overall info from the dataset . ## 1 Reading dataset - Toronto toronto_df = pd.read_csv(&quot;textdata/Toronto-dataset.csv&quot;) toronto_df_text = toronto_df[[&#39;text&#39;]].convert_dtypes(object,str) ## And if we need hashtags as well #df_text = toronto_df[[&#39;hashtags&#39;,&#39;text&#39;]] ## 2 Reading dataset - Chicago chicago_df = pd.read_csv(&quot;textdata/Chicago-dataset.csv&quot;) chicago_df_text = chicago_df[[&#39;text&#39;]].convert_dtypes(object,str) ## 3 Reading dataset - Dallas dallas_df = pd.read_csv(&quot;textdata/Dallas-dataset.csv&quot;) dallas_df_text = dallas_df[[&#39;text&#39;]].convert_dtypes(object,str) ## 4 Reading dataset - Houston houston_df = pd.read_csv(&quot;textdata/Houston-dataset.csv&quot;) houston_df_text = houston_df[[&#39;text&#39;]].convert_dtypes(object,str) ## 5 Reading dataset - LosAngeles losangeles_df = pd.read_csv(&quot;textdata/LosAngeles-dataset.csv&quot;) losangeles_df_text = losangeles_df[[&#39;text&#39;]].convert_dtypes(object,str) ## 6 Reading dataset - NewYork newyork_df = pd.read_csv(&quot;textdata/NewYork-dataset.csv&quot;) newyork_df_text = newyork_df[[&#39;text&#39;]].convert_dtypes(object,str) ## 7 Reading dataset - Philadelphia philadelphia_df = pd.read_csv(&quot;textdata/Philadelphia-dataset.csv&quot;) philadelphia_df_text = philadelphia_df[[&#39;text&#39;]].convert_dtypes(object,str) ## 8 Reading dataset - Phoenix phoenix_df = pd.read_csv(&quot;textdata/Phoenix-dataset.csv&quot;) phoenix_df_text = phoenix_df[[&#39;text&#39;]].convert_dtypes(object,str) ## 9 Reading dataset - SanAntonio sanantonio_df = pd.read_csv(&quot;textdata/SanAntonio-dataset.csv&quot;) sanantonio_df_text = sanantonio_df[[&#39;text&#39;]].convert_dtypes(object,str) ## 10 Reading dataset - SanDiego sandiego_df = pd.read_csv(&quot;textdata/SanDiego-dataset.csv&quot;) sandiego_df_text = sandiego_df[[&#39;text&#39;]].convert_dtypes(object,str) . Checking the dataframe . We look at the over structure of the dataset We display a small number of records to get a sense of the data . . #toronto_df.info() #toronto_df.head(5) # Create a dataframe with &#39;Text&#39; column for tweets toronto_df_text = toronto_df[[&#39;text&#39;]].convert_dtypes(object,str) toronto_df_text[&#39;created_at&#39;] = toronto_df[[&#39;created_at&#39;]].convert_dtypes(object,str) toronto_df_text[&#39;place&#39;] = toronto_df[[&#39;place&#39;]].convert_dtypes(object,str) toronto_df_text[&#39;hashtags&#39;] = toronto_df[[&#39;hashtags&#39;]].convert_dtypes(object,str) . # Create a dataframe with &#39;Text&#39; column for tweets chicago_df_text = chicago_df[[&#39;text&#39;]].convert_dtypes(object,str) chicago_df_text[&#39;created_at&#39;] = chicago_df[[&#39;created_at&#39;]].convert_dtypes(object,str) chicago_df_text[&#39;place&#39;] = chicago_df[[&#39;place&#39;]].convert_dtypes(object,str) chicago_df_text[&#39;hashtags&#39;] = chicago_df[[&#39;hashtags&#39;]].convert_dtypes(object,str) . # Create a dataframe with &#39;Text&#39; column for tweets dallas_df_text = dallas_df[[&#39;text&#39;]].convert_dtypes(object,str) dallas_df_text[&#39;created_at&#39;] = dallas_df[[&#39;created_at&#39;]].convert_dtypes(object,str) dallas_df_text[&#39;place&#39;] = dallas_df[[&#39;place&#39;]].convert_dtypes(object,str) dallas_df_text[&#39;hashtags&#39;] = dallas_df[[&#39;hashtags&#39;]].convert_dtypes(object,str) . # Create a dataframe with &#39;Text&#39; column for tweets houston_df_text = houston_df[[&#39;text&#39;]].convert_dtypes(object,str) houston_df_text[&#39;created_at&#39;] = houston_df[[&#39;created_at&#39;]].convert_dtypes(object,str) houston_df_text[&#39;place&#39;] = houston_df[[&#39;place&#39;]].convert_dtypes(object,str) houston_df_text[&#39;hashtags&#39;] = houston_df[[&#39;hashtags&#39;]].convert_dtypes(object,str) . # Create a dataframe with &#39;Text&#39; column for tweets losangeles_df_text = losangeles_df[[&#39;text&#39;]].convert_dtypes(object,str) losangeles_df_text[&#39;created_at&#39;] = losangeles_df[[&#39;created_at&#39;]].convert_dtypes(object,str) losangeles_df_text[&#39;place&#39;] = losangeles_df[[&#39;place&#39;]].convert_dtypes(object,str) losangeles_df_text[&#39;hashtags&#39;] = losangeles_df[[&#39;hashtags&#39;]].convert_dtypes(object,str) . # Create a dataframe with &#39;Text&#39; column for tweets newyork_df_text = newyork_df[[&#39;text&#39;]].convert_dtypes(object,str) newyork_df_text[&#39;created_at&#39;] = newyork_df[[&#39;created_at&#39;]].convert_dtypes(object,str) newyork_df_text[&#39;place&#39;] = newyork_df[[&#39;place&#39;]].convert_dtypes(object,str) newyork_df_text[&#39;hashtags&#39;] = newyork_df[[&#39;hashtags&#39;]].convert_dtypes(object,str) . # Create a dataframe with &#39;Text&#39; column for tweets philadelphia_df_text = philadelphia_df[[&#39;text&#39;]].convert_dtypes(object,str) philadelphia_df_text[&#39;created_at&#39;] = philadelphia_df[[&#39;created_at&#39;]].convert_dtypes(object,str) philadelphia_df_text[&#39;place&#39;] = philadelphia_df[[&#39;place&#39;]].convert_dtypes(object,str) philadelphia_df_text[&#39;hashtags&#39;] = philadelphia_df[[&#39;hashtags&#39;]].convert_dtypes(object,str) . # Create a dataframe with &#39;Text&#39; column for tweets phoenix_df_text = phoenix_df[[&#39;text&#39;]].convert_dtypes(object,str) phoenix_df_text[&#39;created_at&#39;] = phoenix_df[[&#39;created_at&#39;]].convert_dtypes(object,str) phoenix_df_text[&#39;place&#39;] = phoenix_df[[&#39;place&#39;]].convert_dtypes(object,str) phoenix_df_text[&#39;hashtags&#39;] = phoenix_df[[&#39;hashtags&#39;]].convert_dtypes(object,str) . # Create a dataframe with &#39;Text&#39; column for tweets sanantonio_df_text = sanantonio_df[[&#39;text&#39;]].convert_dtypes(object,str) sanantonio_df_text[&#39;created_at&#39;] = sanantonio_df[[&#39;created_at&#39;]].convert_dtypes(object,str) sanantonio_df_text[&#39;place&#39;] = sanantonio_df[[&#39;place&#39;]].convert_dtypes(object,str) sanantonio_df_text[&#39;hashtags&#39;] = sanantonio_df[[&#39;hashtags&#39;]].convert_dtypes(object,str) . # Create a dataframe with &#39;Text&#39; column for tweets sandiego_df_text = sandiego_df[[&#39;text&#39;]].convert_dtypes(object,str) sandiego_df_text[&#39;created_at&#39;] = sandiego_df[[&#39;created_at&#39;]].convert_dtypes(object,str) sandiego_df_text[&#39;place&#39;] = sandiego_df[[&#39;place&#39;]].convert_dtypes(object,str) sandiego_df_text[&#39;hashtags&#39;] = sandiego_df[[&#39;hashtags&#39;]].convert_dtypes(object,str) . . Step 2: Cleaning the text Data . For this notebook, I&#39;ll be working with the text content of the tweets, so we need to separate and remove . The emojis in between the text | Symbols and numbers | The | . def cleanEmoji(text): regrex_pattern = re.compile(pattern = &quot;[&quot; u&quot; U0001F600- U0001F64F&quot; # emoticons u&quot; U0001F300- U0001F5FF&quot; # symbols &amp; pictographs u&quot; U0001F680- U0001F6FF&quot; # transport &amp; map symbols u&quot; U0001F1E0- U0001F1FF&quot; # flags (iOS) u&quot; U00002702- U000027B0&quot; u&quot; U000024C2- U0001F251&quot; &quot;]+&quot;, flags = re.UNICODE) return regrex_pattern.sub(r&#39;&#39;,text) #2-CleanTweets function replaces non-text entities with space def cleanTweets(text): text = re.sub(r&#39;@[A-Za-z0-9]&#39;,&#39;&#39;,text, flags=re.MULTILINE) # Remove @mentions text = re.sub(r&#39;#&#39;,&#39;&#39;,text, flags=re.MULTILINE) # Removing the # symbol text = re.sub(r&#39;@&#39;,&#39;&#39;,text, flags=re.MULTILINE) # Removing the # symbol text = re.sub(r&#39;RT[ s]+&#39;,&#39;&#39;,text, flags=re.MULTILINE) # Remove RT text = re.sub(r&#39;https?: / / S+&#39;,&#39;&#39;,text, flags=re.MULTILINE) # Remove the hyper link text = re.sub(r&#39;http?: / / S+&#39;,&#39;&#39;,text, flags=re.MULTILINE) # Remove the hyper link text = re.sub(r&#39;^https?: / /.*[ r n]*&#39;, &#39;&#39;, text, flags=re.MULTILINE) # Note: remove special charactres and punctuations - text = re.sub(r&#39;[^a-zA-Z# ]&#39;,&#39;&#39;,text, flags=re.MULTILINE) #text = re.sub(&#39;https&#39;, &#39;&#39;, text, flags=re.MULTILINE) #text = re.sub(&#39;https&#39;, &#39;&#39;, text, flags=re.MULTILINE) text = re.sub(&#39;urbanstreetphotography&#39;, &#39;&#39;, text, flags=re.MULTILINE) text = re.sub(&#39;igstreet&#39;, &#39;&#39;, text, flags=re.MULTILINE) text = re.sub(&#39;cityscape&#39;, &#39;&#39;, text, flags=re.MULTILINE) text = re.sub(&#39;streetphotographer&#39;, &#39;&#39;, text, flags=re.MULTILINE) text = re.sub(&#39;urbanstreetphotogallery&#39;, &#39;&#39;, text, flags=re.MULTILINE) text = re.sub(&#39;photodocumentary&#39;, &#39;&#39;, text, flags=re.MULTILINE) text = re.sub(&#39;ig_street&#39;, &#39;&#39;, text, flags=re.MULTILINE) text = re.sub(&#39;Covid&#39;, &#39;&#39;, text, flags=re.MULTILINE) text = re.sub(&#39;COVID&#39;, &#39;&#39;, text, flags=re.MULTILINE) text = re.sub(&#39;covid&#39;, &#39;&#39;, text, flags=re.MULTILINE) text = re.sub(&#39;spicollective&#39;, &#39;&#39;, text, flags=re.MULTILINE) text = re.sub(&#39;Spicollective&#39;, &#39;&#39;, text, flags=re.MULTILINE) text = re.sub(&#39;lensculture&#39;, &#39;&#39;, text, flags=re.MULTILINE) #text = re.sub(&#39;Toronto&#39;, &#39;&#39;, text, flags=re.MULTILINE) #text = re.sub(&#39;Ontario&#39;, &#39;&#39;, text, flags=re.MULTILINE) text = re.sub(&#39;will&#39;, &#39;&#39;, text, flags=re.MULTILINE) #text = re.sub(&#39;bnw&#39;, &#39;&#39;, text, flags=re.MULTILINE) text = re.sub(&#39;bw&#39;, &#39;&#39;, text, flags=re.MULTILINE) text = re.sub(&#39;thi&#39;, &#39;&#39;, text, flags=re.MULTILINE) text = re.sub(&#39;wa&#39;, &#39;&#39;, text, flags=re.MULTILINE) text = re.sub(r&#39; xc2 xb7&#39;,&#39;&#39;,text) # Remove bullet points text = re.sub(r&#39; u2022&#39;,&#39;&#39;,text) # Remove bullet points # text = re.sub(r&#39;https?: / /(www .)?[-a-zA-Z0–9@:%._ +~#=]{2,256} .[a-z]{2,6} b([-a-zA-Z0–9@:%_ +.~#?&amp;//=]*)&#39;, # &#39;&#39;, text, flags=re.MULTILINE) # Remove links that start with HTTP/HTTPS in the tweet # text = re.sub(r&#39;[-a-zA-Z0–9@:%._ +~#=]{2,256} .[a-z]{2,6} b([-a-zA-Z0–9@:%_ +.~#?&amp;//=]*)&#39;, # &#39;&#39;, text, flags=re.MULTILINE) # Remove other url links return text . # s = toronto_df_text.iloc[[2]] toronto_df_text[&#39;text&#39;] = toronto_df_text[&#39;text&#39;].apply(cleanTweets) toronto_df_text[&#39;text&#39;] = toronto_df_text[&#39;text&#39;].apply(cleanEmoji) #toronto_df_text.head(2) . chicago_df_text[&#39;text&#39;] = chicago_df_text[&#39;text&#39;].apply(cleanTweets) chicago_df_text[&#39;text&#39;] = chicago_df_text[&#39;text&#39;].apply(cleanEmoji) . dallas_df_text[&#39;text&#39;] = dallas_df_text[&#39;text&#39;].apply(cleanTweets) dallas_df_text[&#39;text&#39;] = dallas_df_text[&#39;text&#39;].apply(cleanEmoji) . houston_df_text[&#39;text&#39;] = houston_df_text[&#39;text&#39;].apply(cleanTweets) houston_df_text[&#39;text&#39;] = houston_df_text[&#39;text&#39;].apply(cleanEmoji) . losangeles_df_text[&#39;text&#39;] = losangeles_df_text[&#39;text&#39;].apply(cleanTweets) losangeles_df_text[&#39;text&#39;] = losangeles_df_text[&#39;text&#39;].apply(cleanEmoji) . newyork_df_text[&#39;text&#39;] = newyork_df_text[&#39;text&#39;].apply(cleanTweets) newyork_df_text[&#39;text&#39;] = newyork_df_text[&#39;text&#39;].apply(cleanEmoji) . philadelphia_df_text[&#39;text&#39;] = philadelphia_df_text[&#39;text&#39;].apply(cleanTweets) philadelphia_df_text[&#39;text&#39;] = philadelphia_df_text[&#39;text&#39;].apply(cleanEmoji) . phoenix_df_text[&#39;text&#39;] = phoenix_df_text[&#39;text&#39;].apply(cleanTweets) phoenix_df_text[&#39;text&#39;] = phoenix_df_text[&#39;text&#39;].apply(cleanEmoji) . sanantonio_df_text[&#39;text&#39;] = sanantonio_df_text[&#39;text&#39;].apply(cleanTweets) sanantonio_df_text[&#39;text&#39;] = sanantonio_df_text[&#39;text&#39;].apply(cleanEmoji) . sandiego_df_text[&#39;text&#39;] = sandiego_df_text[&#39;text&#39;].apply(cleanTweets) sandiego_df_text[&#39;text&#39;] = sandiego_df_text[&#39;text&#39;].apply(cleanEmoji) . A- Sentence Level Analysis . Start working on the contect and subject area . def getSubjectivity(text): return TextBlob(text).sentiment.subjectivity #create a function to get the polarity def getPolarity(text): return TextBlob(text).sentiment.polarity . toronto_df_text[&#39;subjectivity&#39;] = toronto_df_text[&#39;text&#39;].apply(getSubjectivity) toronto_df_text[&#39;polarity&#39;] = toronto_df_text[&#39;text&#39;].apply(getPolarity) . chicago_df_text[&#39;subjectivity&#39;] = chicago_df_text[&#39;text&#39;].apply(getSubjectivity) chicago_df_text[&#39;polarity&#39;] = chicago_df_text[&#39;text&#39;].apply(getPolarity) . dallas_df_text[&#39;subjectivity&#39;] = dallas_df_text[&#39;text&#39;].apply(getSubjectivity) dallas_df_text[&#39;polarity&#39;] = dallas_df_text[&#39;text&#39;].apply(getPolarity) . houston_df_text[&#39;subjectivity&#39;] = houston_df_text[&#39;text&#39;].apply(getSubjectivity) houston_df_text[&#39;polarity&#39;] = houston_df_text[&#39;text&#39;].apply(getPolarity) . losangeles_df_text[&#39;subjectivity&#39;] = losangeles_df_text[&#39;text&#39;].apply(getSubjectivity) losangeles_df_text[&#39;polarity&#39;] = losangeles_df_text[&#39;text&#39;].apply(getPolarity) . newyork_df_text[&#39;subjectivity&#39;] = newyork_df_text[&#39;text&#39;].apply(getSubjectivity) newyork_df_text[&#39;polarity&#39;] = newyork_df_text[&#39;text&#39;].apply(getPolarity) . philadelphia_df_text[&#39;subjectivity&#39;] = philadelphia_df_text[&#39;text&#39;].apply(getSubjectivity) philadelphia_df_text[&#39;polarity&#39;] = philadelphia_df_text[&#39;text&#39;].apply(getPolarity) . phoenix_df_text[&#39;subjectivity&#39;] = phoenix_df_text[&#39;text&#39;].apply(getSubjectivity) phoenix_df_text[&#39;polarity&#39;] = phoenix_df_text[&#39;text&#39;].apply(getPolarity) . sanantonio_df_text[&#39;subjectivity&#39;] = sanantonio_df_text[&#39;text&#39;].apply(getSubjectivity) sanantonio_df_text[&#39;polarity&#39;] = sanantonio_df_text[&#39;text&#39;].apply(getPolarity) . sandiego_df_text[&#39;subjectivity&#39;] = sandiego_df_text[&#39;text&#39;].apply(getSubjectivity) sandiego_df_text[&#39;polarity&#39;] = sandiego_df_text[&#39;text&#39;].apply(getPolarity) . Make a WordCloud chart -word frequency map . # visualize the most frequency used words # prepare the tweet data allWords_toronto = &#39; &#39;.join(twts for twts in toronto_df_text[&#39;text&#39;]) allWords_chicago = &#39; &#39;.join(twts for twts in chicago_df_text[&#39;text&#39;]) allWords_dallas = &#39; &#39;.join(twts for twts in dallas_df_text[&#39;text&#39;]) allWords_houston = &#39; &#39;.join(twts for twts in houston_df_text[&#39;text&#39;]) allWords_losangeles = &#39; &#39;.join(twts for twts in losangeles_df_text[&#39;text&#39;]) allWords_newyork = &#39; &#39;.join(twts for twts in newyork_df_text[&#39;text&#39;]) allWords_philadelphia = &#39; &#39;.join(twts for twts in philadelphia_df_text[&#39;text&#39;]) allWords_phoenix = &#39; &#39;.join(twts for twts in phoenix_df_text[&#39;text&#39;]) allWords_sanantonio = &#39; &#39;.join(twts for twts in sanantonio_df_text[&#39;text&#39;]) allWords_sandiego = &#39; &#39;.join(twts for twts in sandiego_df_text[&#39;text&#39;]) wordCloud_toronto = WordCloud(width=800, height=500, random_state=21,max_font_size=119).generate(allWords_toronto) wordCloud_chicago = WordCloud(width=800, height=500, random_state=21,max_font_size=119).generate(allWords_chicago) wordCloud_dallas = WordCloud(width=800, height=500, random_state=21,max_font_size=119).generate(allWords_dallas) wordCloud_houston = WordCloud(width=800, height=500, random_state=21,max_font_size=119).generate(allWords_houston) wordCloud_losangeles = WordCloud(width=800, height=500, random_state=21,max_font_size=119).generate(allWords_losangeles) wordCloud_newyork = WordCloud(width=800, height=500, random_state=21,max_font_size=119).generate(allWords_newyork) wordCloud_philadelphia = WordCloud(width=800, height=500, random_state=21,max_font_size=119).generate(allWords_philadelphia) wordCloud_phoenix = WordCloud(width=800, height=500, random_state=21,max_font_size=119).generate(allWords_phoenix) wordCloud_sanantonio = WordCloud(width=800, height=500, random_state=21,max_font_size=119).generate(allWords_sanantonio) wordCloud_sandiego = WordCloud(width=800, height=500, random_state=21,max_font_size=119).generate(allWords_sandiego) # 1 - Toronto wordcloud plt.figure(figsize=(19,8)) plt.suptitle(&#39;Toronto&#39;) plt.imshow(wordCloud_toronto, interpolation= &#39;bilinear&#39;) plt.axis(&#39;off&#39;) plt.show() # 2- Chicago wordcloud plt.figure(figsize=(19,8)) plt.suptitle(&#39;Chicago&#39;) plt.imshow(wordCloud_chicago, interpolation= &#39;bilinear&#39;) plt.axis(&#39;off&#39;) plt.show() # 3- Dallas wordcloud plt.figure(figsize=(19,8)) plt.suptitle(&#39;Dallas&#39;) plt.imshow(wordCloud_dallas, interpolation= &#39;bilinear&#39;) plt.axis(&#39;off&#39;) plt.show() # 4- Houston wordcloud plt.figure(figsize=(19,8)) plt.suptitle(&#39;Houston&#39;) plt.imshow(wordCloud_houston, interpolation= &#39;bilinear&#39;) plt.axis(&#39;off&#39;) plt.show() # 5- LosAngeles wordcloud plt.figure(figsize=(19,8)) plt.suptitle(&#39;LosAngeles&#39;) plt.imshow(wordCloud_losangeles, interpolation= &#39;bilinear&#39;) plt.axis(&#39;off&#39;) plt.show() # 2- NewYork wordcloud plt.figure(figsize=(19,8)) plt.suptitle(&#39;NewYork&#39;) plt.imshow(wordCloud_newyork, interpolation= &#39;bilinear&#39;) plt.axis(&#39;off&#39;) plt.show() # 2- Philadelphia wordcloud plt.figure(figsize=(19,8)) plt.suptitle(&#39;Philadelphia&#39;) plt.imshow(wordCloud_philadelphia, interpolation= &#39;bilinear&#39;) plt.axis(&#39;off&#39;) plt.show() # 2- Phoenix wordcloud plt.figure(figsize=(19,8)) plt.suptitle(&#39;Phoenix&#39;) plt.imshow(wordCloud_phoenix, interpolation= &#39;bilinear&#39;) plt.axis(&#39;off&#39;) plt.show() # 2- SanAntonio wordcloud plt.figure(figsize=(19,8)) plt.suptitle(&#39;SanAntonio&#39;) plt.imshow(wordCloud_sanantonio, interpolation= &#39;bilinear&#39;) plt.axis(&#39;off&#39;) plt.show() # 2- SanDiego wordcloud plt.figure(figsize=(19,8)) plt.suptitle(&#39;SanDiego&#39;) plt.imshow(wordCloud_sandiego, interpolation= &#39;bilinear&#39;) plt.axis(&#39;off&#39;) plt.show() . Positive and Negative Sentiments . def getSentAnalysis(score): if score &lt; 0: return &quot;Negative&quot; elif score == 0: return &#39;Neutral&#39; else: return &#39;Positive&#39; # Add column analysis and add a label such as positive-negative-neutral to each tweet toronto_df_text[&#39;analysis&#39;] = toronto_df_text[&#39;polarity&#39;].apply(getSentAnalysis) chicago_df_text[&#39;analysis&#39;] = chicago_df_text[&#39;polarity&#39;].apply(getSentAnalysis) dallas_df_text[&#39;analysis&#39;] = dallas_df_text[&#39;polarity&#39;].apply(getSentAnalysis) houston_df_text[&#39;analysis&#39;] = houston_df_text[&#39;polarity&#39;].apply(getSentAnalysis) losangeles_df_text[&#39;analysis&#39;] = losangeles_df_text[&#39;polarity&#39;].apply(getSentAnalysis) newyork_df_text[&#39;analysis&#39;] = newyork_df_text[&#39;polarity&#39;].apply(getSentAnalysis) philadelphia_df_text[&#39;analysis&#39;] = philadelphia_df_text[&#39;polarity&#39;].apply(getSentAnalysis) phoenix_df_text[&#39;analysis&#39;] = phoenix_df_text[&#39;polarity&#39;].apply(getSentAnalysis) sanantonio_df_text[&#39;analysis&#39;] = sanantonio_df_text[&#39;polarity&#39;].apply(getSentAnalysis) sandiego_df_text[&#39;analysis&#39;] = sandiego_df_text[&#39;polarity&#39;].apply(getSentAnalysis) . All Positive/Negative Tweets . skip this one . #print all of the negative tweets - Toronto j=1 sortedDF_text = toronto_df_text.sort_values(by=[&#39;polarity&#39;]) for i in range(0, sortedDF_text.shape[0]): if (sortedDF_text[&#39;analysis&#39;][i] == &#39;Negative&#39;): # print(str(j)+ &#39;)&#39; + sortedDF_text[&#39;text&#39;][i]) # print() j = j+1 #print all of the Positive tweets j=1 sortedDF_text = toronto_df_text.sort_values(by=[&#39;polarity&#39;]) for i in range(0, sortedDF_text.shape[0]): if (sortedDF_text[&#39;analysis&#39;][i] == &#39;Positive&#39;): # print(str(j)+ &#39;)&#39; + sortedDF_text[&#39;text&#39;][i]) # print() j = j+1 . skip this one . j=1 sortedDF_text = chicago_df_text.sort_values(by=[&#39;polarity&#39;]) for i in range(0, sortedDF_text.shape[0]): if (sortedDF_text[&#39;analysis&#39;][i] == &#39;Negative&#39;): # print(str(j)+ &#39;)&#39; + sortedDF_text[&#39;text&#39;][i]) # print() j = j+1 #print all of the Positive tweets j=1 sortedDF_text = chicago_df_text.sort_values(by=[&#39;polarity&#39;]) for i in range(0, sortedDF_text.shape[0]): if (sortedDF_text[&#39;analysis&#39;][i] == &#39;Positive&#39;): # print(str(j)+ &#39;)&#39; + sortedDF_text[&#39;text&#39;][i]) # print() j = j+1 . Scatter pLot - map the postitive-negative distribution . # 1 - Toronto Sentiment Plot plt.figure(figsize=(12,8)) for i in range(0,toronto_df_text.shape[0]): plt.scatter(toronto_df_text[&#39;polarity&#39;][i],toronto_df_text[&#39;subjectivity&#39;][i], color=&#39;Blue&#39;) plt.title(&#39;Toronto Sentiment Analysis&#39;) plt.xlabel(&#39;Subjectivity&#39;) plt.xlabel(&#39;Polarity&#39;) plt.show() # 2 - Chicago Sentiment Plot plt.figure(figsize=(12,8)) for i in range(0,chicago_df_text.shape[0]): plt.scatter(chicago_df_text[&#39;polarity&#39;][i],chicago_df_text[&#39;subjectivity&#39;][i], color=&#39;Blue&#39;) plt.title(&#39;Chicago Sentiment Analysis&#39;) plt.xlabel(&#39;Subjectivity&#39;) plt.xlabel(&#39;Polarity&#39;) plt.show() # 3 - Dallas Sentiment Plot plt.figure(figsize=(12,8)) for i in range(0,dallas_df_text.shape[0]): plt.scatter(dallas_df_text[&#39;polarity&#39;][i],dallas_df_text[&#39;subjectivity&#39;][i], color=&#39;Blue&#39;) plt.title(&#39;Dallas Sentiment Analysis&#39;) plt.xlabel(&#39;Subjectivity&#39;) plt.xlabel(&#39;Polarity&#39;) plt.show() # 4 - Houston Sentiment Plot plt.figure(figsize=(12,8)) for i in range(0,houston_df_text.shape[0]): plt.scatter(houston_df_text[&#39;polarity&#39;][i],houston_df_text[&#39;subjectivity&#39;][i], color=&#39;Blue&#39;) plt.title(&#39;Houston Sentiment Analysis&#39;) plt.xlabel(&#39;Subjectivity&#39;) plt.xlabel(&#39;Polarity&#39;) plt.show() # 5 - LosAngeles Sentiment Plot plt.figure(figsize=(12,8)) for i in range(0,losangeles_df_text.shape[0]): plt.scatter(losangeles_df_text[&#39;polarity&#39;][i],losangeles_df_text[&#39;subjectivity&#39;][i], color=&#39;Blue&#39;) plt.title(&#39;LosAngeles Sentiment Analysis&#39;) plt.xlabel(&#39;Subjectivity&#39;) plt.xlabel(&#39;Polarity&#39;) plt.show() # 6 - NewYork Sentiment Plot plt.figure(figsize=(12,8)) for i in range(0,newyork_df_text.shape[0]): plt.scatter(newyork_df_text[&#39;polarity&#39;][i],newyork_df_text[&#39;subjectivity&#39;][i], color=&#39;Blue&#39;) plt.title(&#39;NewYork Sentiment Analysis&#39;) plt.xlabel(&#39;Subjectivity&#39;) plt.xlabel(&#39;Polarity&#39;) plt.show() # 7 - Philadelphia Sentiment Plot plt.figure(figsize=(12,8)) for i in range(0,philadelphia_df_text.shape[0]): plt.scatter(philadelphia_df_text[&#39;polarity&#39;][i],philadelphia_df_text[&#39;subjectivity&#39;][i], color=&#39;Blue&#39;) plt.title(&#39;Philadelphia Sentiment Analysis&#39;) plt.xlabel(&#39;Subjectivity&#39;) plt.xlabel(&#39;Polarity&#39;) plt.show() # 8 - Phoenix Sentiment Plot plt.figure(figsize=(12,8)) for i in range(0,phoenix_df_text.shape[0]): plt.scatter(phoenix_df_text[&#39;polarity&#39;][i],phoenix_df_text[&#39;subjectivity&#39;][i], color=&#39;Blue&#39;) plt.title(&#39;Phoenix Sentiment Analysis&#39;) plt.xlabel(&#39;Subjectivity&#39;) plt.xlabel(&#39;Polarity&#39;) plt.show() # 9 - SanAntonio Sentiment Plot plt.figure(figsize=(12,8)) for i in range(0,sanantonio_df_text.shape[0]): plt.scatter(sanantonio_df_text[&#39;polarity&#39;][i],sanantonio_df_text[&#39;subjectivity&#39;][i], color=&#39;Blue&#39;) plt.title(&#39;SanAntonio Sentiment Analysis&#39;) plt.xlabel(&#39;Subjectivity&#39;) plt.xlabel(&#39;Polarity&#39;) plt.show() # 10 - SanDiego Sentiment Plot plt.figure(figsize=(12,8)) for i in range(0,sandiego_df_text.shape[0]): plt.scatter(sandiego_df_text[&#39;polarity&#39;][i],sandiego_df_text[&#39;subjectivity&#39;][i], color=&#39;Blue&#39;) plt.title(&#39;SanDiego Sentiment Analysis&#39;) plt.xlabel(&#39;Subjectivity&#39;) plt.xlabel(&#39;Polarity&#39;) plt.show() . Overall picture: Positive, Negative, Neutral . # 1 Toronto Positive-Negative Tweets # Percentage of Positive Tweets ptweets = toronto_df_text[toronto_df_text.analysis == &#39;Positive&#39;] ptweets = ptweets [&#39;text&#39;] positivetweets = round ((ptweets.shape[0] /toronto_df_text.shape[0]) *100,1) print (f&quot;--&gt; percentage of positive tweets: {positivetweets} %&quot;) # Get the percentage of negative tweets ntweets = toronto_df_text[toronto_df_text.analysis == &#39;Negative&#39;] ntweets = ntweets[&#39;text&#39;] # Percentage of Negative Tweets negativetweets = round( (ntweets.shape[0] / toronto_df_text.shape[0]*100),1) print (f&quot;--&gt; percentage of negative tweets: {negativetweets} %&quot;) #Show the value counts toronto_df_text[&#39;analysis&#39;].value_counts() . --&gt; percentage of positive tweets: 50.3 % --&gt; percentage of negative tweets: 14.4 % --&gt; percentage of positive tweets: 53.0 % --&gt; percentage of negative tweets: 16.9 % . Positive 1634 Neutral 927 Negative 521 Name: analysis, dtype: int64 . # Percentage of Positive Tweets ptweets = chicago_df_text[chicago_df_text.analysis == &#39;Positive&#39;] ptweets = ptweets [&#39;text&#39;] positivetweets = round ((ptweets.shape[0] /chicago_df_text.shape[0]) *100,1) print (f&quot;--&gt; percentage of positive tweets: {positivetweets} %&quot;) # Get the percentage of negative tweets ntweets = chicago_df_text[chicago_df_text.analysis == &#39;Negative&#39;] ntweets = ntweets[&#39;text&#39;] # Percentage of Negative Tweets negativetweets = round( (ntweets.shape[0] / chicago_df_text.shape[0]*100),1) print (f&quot;--&gt; percentage of negative tweets: {negativetweets} %&quot;) #Show the value counts chicago_df_text[&#39;analysis&#39;].value_counts() # 3 Dallas Positive-Negative Tweets # Percentage of Positive Tweets ptweets = dallas_df_text[dallas_df_text.analysis == &#39;Positive&#39;] ptweets = ptweets [&#39;text&#39;] positivetweets = round ((ptweets.shape[0] /dallas_df_text.shape[0]) *100,1) print (f&quot;--&gt; percentage of positive tweets: {positivetweets} %&quot;) # Get the percentage of negative tweets ntweets = dallas_df_text[dallas_df_text.analysis == &#39;Negative&#39;] ntweets = ntweets[&#39;text&#39;] # Percentage of Negative Tweets negativetweets = round( (ntweets.shape[0] / dallas_df_text.shape[0]*100),1) print (f&quot;--&gt; percentage of negative tweets: {negativetweets} %&quot;) #Show the value counts dallas_df_text[&#39;analysis&#39;].value_counts() # 4 Houston Positive-Negative Tweets # Percentage of Positive Tweets ptweets = houston_df_text[houston_df_text.analysis == &#39;Positive&#39;] ptweets = ptweets [&#39;text&#39;] positivetweets = round ((ptweets.shape[0] /houston_df_text.shape[0]) *100,1) print (f&quot;--&gt; percentage of positive tweets: {positivetweets} %&quot;) # Get the percentage of negative tweets ntweets = houston_df_text[chicago_df_text.analysis == &#39;Negative&#39;] ntweets = ntweets[&#39;text&#39;] # Percentage of Negative Tweets negativetweets = round( (ntweets.shape[0] / houston_df_text.shape[0]*100),1) print (f&quot;--&gt; percentage of negative tweets: {negativetweets} %&quot;) #Show the value counts houston_df_text[&#39;analysis&#39;].value_counts() # 5 LosAngeles Positive-Negative Tweets # Percentage of Positive Tweets ptweets = losangeles_df_text[losangeles_df_text.analysis == &#39;Positive&#39;] ptweets = ptweets [&#39;text&#39;] positivetweets = round ((ptweets.shape[0] /losangeles_df_text.shape[0]) *100,1) print (f&quot;--&gt; percentage of positive tweets: {positivetweets} %&quot;) # Get the percentage of negative tweets ntweets = losangeles_df_text[losangeles_df_text.analysis == &#39;Negative&#39;] ntweets = ntweets[&#39;text&#39;] # Percentage of Negative Tweets negativetweets = round( (ntweets.shape[0] / losangeles_df_text.shape[0]*100),1) print (f&quot;--&gt; percentage of negative tweets: {negativetweets} %&quot;) #Show the value counts losangeles_df_text[&#39;analysis&#39;].value_counts() # 6 NewYork Positive-Negative Tweets # Percentage of Positive Tweets ptweets = newyork_df_text[newyork_df_text.analysis == &#39;Positive&#39;] ptweets = ptweets [&#39;text&#39;] positivetweets = round ((ptweets.shape[0] /newyork_df_text.shape[0]) *100,1) print (f&quot;--&gt; percentage of positive tweets: {positivetweets} %&quot;) # Get the percentage of negative tweets ntweets = newyork_df_text[newyork_df_text.analysis == &#39;Negative&#39;] ntweets = ntweets[&#39;text&#39;] # Percentage of Negative Tweets negativetweets = round( (ntweets.shape[0] / newyork_df_text.shape[0]*100),1) print (f&quot;--&gt; percentage of negative tweets: {negativetweets} %&quot;) #Show the value counts newyork_df_text[&#39;analysis&#39;].value_counts() # 7 Philadelphia Positive-Negative Tweets # Percentage of Positive Tweets ptweets = philadelphia_df_text[philadelphia_df_text.analysis == &#39;Positive&#39;] ptweets = ptweets [&#39;text&#39;] positivetweets = round ((ptweets.shape[0] /philadelphia_df_text.shape[0]) *100,1) print (f&quot;--&gt; percentage of positive tweets: {positivetweets} %&quot;) # Get the percentage of negative tweets ntweets = philadelphia_df_text[philadelphia_df_text.analysis == &#39;Negative&#39;] ntweets = ntweets[&#39;text&#39;] # Percentage of Negative Tweets negativetweets = round( (ntweets.shape[0] / chicago_df_text.shape[0]*100),1) print (f&quot;--&gt; percentage of negative tweets: {negativetweets} %&quot;) #Show the value counts philadelphia_df_text[&#39;analysis&#39;].value_counts() # 8 Phoenix Positive-Negative Tweets # Percentage of Positive Tweets ptweets = phoenix_df_text[phoenix_df_text.analysis == &#39;Positive&#39;] ptweets = ptweets [&#39;text&#39;] positivetweets = round ((ptweets.shape[0] /phoenix_df_text.shape[0]) *100,1) print (f&quot;--&gt; percentage of positive tweets: {positivetweets} %&quot;) # Get the percentage of negative tweets ntweets = phoenix_df_text[phoenix_df_text.analysis == &#39;Negative&#39;] ntweets = ntweets[&#39;text&#39;] # Percentage of Negative Tweets negativetweets = round( (ntweets.shape[0] / phoenix_df_text.shape[0]*100),1) print (f&quot;--&gt; percentage of negative tweets: {negativetweets} %&quot;) #Show the value counts phoenix_df_text[&#39;analysis&#39;].value_counts() # 9 SanAntonio Positive-Negative Tweets # Percentage of Positive Tweets ptweets = sanantonio_df_text[sanantonio_df_text.analysis == &#39;Positive&#39;] ptweets = ptweets [&#39;text&#39;] positivetweets = round ((ptweets.shape[0] /sanantonio_df_text.shape[0]) *100,1) print (f&quot;--&gt; percentage of positive tweets: {positivetweets} %&quot;) # Get the percentage of negative tweets ntweets = sanantonio_df_text[sanantonio_df_text.analysis == &#39;Negative&#39;] ntweets = ntweets[&#39;text&#39;] # Percentage of Negative Tweets negativetweets = round( (ntweets.shape[0] / sanantonio_df_text.shape[0]*100),1) print (f&quot;--&gt; percentage of negative tweets: {negativetweets} %&quot;) #Show the value counts sanantonio_df_text[&#39;analysis&#39;].value_counts() # 10 SanDiego Positive-Negative Tweets # Percentage of Positive Tweets ptweets = sandiego_df_text[sandiego_df_text.analysis == &#39;Positive&#39;] ptweets = ptweets [&#39;text&#39;] positivetweets = round ((ptweets.shape[0] /sandiego_df_text.shape[0]) *100,1) print (f&quot;--&gt; percentage of positive tweets: {positivetweets} %&quot;) # Get the percentage of negative tweets ntweets = sandiego_df_text[sandiego_df_text.analysis == &#39;Negative&#39;] ntweets = ntweets[&#39;text&#39;] # Percentage of Negative Tweets negativetweets = round( (ntweets.shape[0] / sandiego_df_text.shape[0]*100),1) print (f&quot;--&gt; percentage of negative tweets: {negativetweets} %&quot;) #Show the value counts sandiego_df_text[&#39;analysis&#39;].value_counts() . --&gt; percentage of positive tweets: 52.9 % --&gt; percentage of negative tweets: 16.8 % --&gt; percentage of positive tweets: 60.0 % --&gt; percentage of negative tweets: 14.4 % --&gt; percentage of positive tweets: 55.1 % --&gt; percentage of negative tweets: 16.7 % --&gt; percentage of positive tweets: 52.2 % --&gt; percentage of negative tweets: 16.9 % --&gt; percentage of positive tweets: 52.9 % --&gt; percentage of negative tweets: 21.1 % --&gt; percentage of positive tweets: 55.0 % --&gt; percentage of negative tweets: 7.2 % --&gt; percentage of positive tweets: 56.6 % --&gt; percentage of negative tweets: 15.7 % --&gt; percentage of positive tweets: 58.3 % --&gt; percentage of negative tweets: 12.6 % --&gt; percentage of positive tweets: 58.5 % --&gt; percentage of negative tweets: 15.7 % . Positive 796 Neutral 350 Negative 214 Name: analysis, dtype: int64 . plt.title(&#39;Toronto - Sentiment Analysis&#39;) plt.xlabel(&#39;Sentiment&#39;) plt.ylabel(&#39;Counts&#39;) toronto_df_text[&#39;analysis&#39;].value_counts().plot(kind=&#39;bar&#39;) plt.show(&#39;Sentiment&#39;) # 2 Chicago plot and visualize the counts plt.title(&#39;Chicago - Sentiment Analysis&#39;) plt.xlabel(&#39;Sentiment&#39;) plt.ylabel(&#39;Counts&#39;) chicago_df_text[&#39;analysis&#39;].value_counts().plot(kind=&#39;bar&#39;) plt.show(&#39;Sentiment&#39;) # 3 Dallas plot and visualize the counts plt.title(&#39;Dallas - Sentiment Analysis&#39;) plt.xlabel(&#39;Sentiment&#39;) plt.ylabel(&#39;Counts&#39;) dallas_df_text[&#39;analysis&#39;].value_counts().plot(kind=&#39;bar&#39;) plt.show(&#39;Sentiment&#39;) # 4 Houston plot and visualize the counts plt.title(&#39;Houston - Sentiment Analysis&#39;) plt.xlabel(&#39;Sentiment&#39;) plt.ylabel(&#39;Counts&#39;) houston_df_text[&#39;analysis&#39;].value_counts().plot(kind=&#39;bar&#39;) plt.show(&#39;Sentiment&#39;) # 5 LosAngeles plot and visualize the counts plt.title(&#39;LosAngeles - Sentiment Analysis&#39;) plt.xlabel(&#39;Sentiment&#39;) plt.ylabel(&#39;Counts&#39;) losangeles_df_text[&#39;analysis&#39;].value_counts().plot(kind=&#39;bar&#39;) plt.show(&#39;Sentiment&#39;) # 6 NewYork plot and visualize the counts plt.title(&#39;NewYork - Sentiment Analysis&#39;) plt.xlabel(&#39;Sentiment&#39;) plt.ylabel(&#39;Counts&#39;) newyork_df_text[&#39;analysis&#39;].value_counts().plot(kind=&#39;bar&#39;) plt.show(&#39;Sentiment&#39;) # 7 Philadelphia plot and visualize the counts plt.title(&#39;Philadelphia - Sentiment Analysis&#39;) plt.xlabel(&#39;Sentiment&#39;) plt.ylabel(&#39;Counts&#39;) philadelphia_df_text[&#39;analysis&#39;].value_counts().plot(kind=&#39;bar&#39;) plt.show(&#39;Sentiment&#39;) # 8 Phoenix plot and visualize the counts plt.title(&#39;Phoenix - Sentiment Analysis&#39;) plt.xlabel(&#39;Sentiment&#39;) plt.ylabel(&#39;Counts&#39;) phoenix_df_text[&#39;analysis&#39;].value_counts().plot(kind=&#39;bar&#39;) plt.show(&#39;Sentiment&#39;) # 9 SanAntonio plot and visualize the counts plt.title(&#39;SanAntonio - Sentiment Analysis&#39;) plt.xlabel(&#39;Sentiment&#39;) plt.ylabel(&#39;Counts&#39;) sanantonio_df_text[&#39;analysis&#39;].value_counts().plot(kind=&#39;bar&#39;) plt.show(&#39;Sentiment&#39;) # 10 SanDiego plot and visualize the counts plt.title(&#39;SanDiego - Sentiment Analysis&#39;) plt.xlabel(&#39;Sentiment&#39;) plt.ylabel(&#39;Counts&#39;) sandiego_df_text[&#39;analysis&#39;].value_counts().plot(kind=&#39;bar&#39;) plt.show(&#39;Sentiment&#39;) . B- Word Level Analysis . B.1 Define Tokenized Tweets . import nltk #nltk.download(&#39;punkt&#39;) #nltk.download(&#39;stopwords&#39;) . from nltk.corpus import stopwords # nltk.download(&#39;stopwords&#39;) #need to run on a new machine import ast #used for literal_eval . ## split content into words toronto_df_text[&#39;tokenized_tw&#39;] = toronto_df_text.apply(lambda row: nltk.word_tokenize(row[&#39;text&#39;]), axis=1) chicago_df_text[&#39;tokenized_tw&#39;] = chicago_df_text.apply(lambda row: nltk.word_tokenize(row[&#39;text&#39;]), axis=1) dallas_df_text[&#39;tokenized_tw&#39;] = dallas_df_text.apply(lambda row: nltk.word_tokenize(row[&#39;text&#39;]), axis=1) houston_df_text[&#39;tokenized_tw&#39;] = houston_df_text.apply(lambda row: nltk.word_tokenize(row[&#39;text&#39;]), axis=1) losangeles_df_text[&#39;tokenized_tw&#39;] = losangeles_df_text.apply(lambda row: nltk.word_tokenize(row[&#39;text&#39;]), axis=1) newyork_df_text[&#39;tokenized_tw&#39;] = newyork_df_text.apply(lambda row: nltk.word_tokenize(row[&#39;text&#39;]), axis=1) philadelphia_df_text[&#39;tokenized_tw&#39;] = philadelphia_df_text.apply(lambda row: nltk.word_tokenize(row[&#39;text&#39;]), axis=1) phoenix_df_text[&#39;tokenized_tw&#39;] = phoenix_df_text.apply(lambda row: nltk.word_tokenize(row[&#39;text&#39;]), axis=1) sanantonio_df_text[&#39;tokenized_tw&#39;] = sanantonio_df_text.apply(lambda row: nltk.word_tokenize(row[&#39;text&#39;]), axis=1) sandiego_df_text[&#39;tokenized_tw&#39;] = sandiego_df_text.apply(lambda row: nltk.word_tokenize(row[&#39;text&#39;]), axis=1) # toronto_df_text[&#39;tokenized_tw&#39;] = toronto_df_text[&#39;tokenized_tw&#39;].tolist() . #toronto_df_text = toronto_df_text.apply(lambda x: x.astype(str).str.lower()) #toronto_df_text.head(1) . B. 2 Apply Filters - Find/Remove Stopwords . # stop_words = stopwords.words(&#39;english&#39;) # check what is included here # stop_words . #type(toronto_df_text[&#39;tokenized_tw&#39;]) . def remove_stopwords(TokenList): # x = &#39;[ &quot;A&quot;,&quot;B&quot;,&quot;C&quot; , &quot; D&quot;]&#39; # TokenList = str(TokenList) # Identify StopWords - get them from stopwords stop_words = stopwords.words(&#39;english&#39;) # convert string value to a list if isinstance(TokenList, str): TokenList = ast.literal_eval(TokenList) # TokenList = TokenList.split(&quot;,&quot;) for i, key in enumerate(TokenList): if (key in stop_words): TokenList [i] = &#39;&#39; return TokenList nRecords = toronto_df_text[&#39;tokenized_tw&#39;].count() # nRecords = 3 # if the tokenized column is string change it to list for x in range(nRecords): if (isinstance(toronto_df_text[&#39;tokenized_tw&#39;][x],str)): # tokenListHere = toronto_df_text[&#39;tokenized_tw&#39;][0].split(&quot;,&quot;) toronto_df_text[&#39;tokenized_tw&#39;][x] = remove_stopwords(toronto_df_text[&#39;tokenized_tw&#39;][x]) . # toronto_df_text[&#39;tokenized_tw&#39;] = toronto_df_text[&#39;tokenized_tw&#39;].apply(lambda x: [item for item in str(x).split() if len(x) &gt; 2]) #toronto_df_text.head(2) . nRecords = chicago_df_text[&#39;tokenized_tw&#39;].count() # nRecords = 3 # if the tokenized column is string change it to list for x in range(nRecords): if (isinstance(chicago_df_text[&#39;tokenized_tw&#39;][x],str)): # tokenListHere = toronto_df_text[&#39;tokenized_tw&#39;][0].split(&quot;,&quot;) chicago_df_text[&#39;tokenized_tw&#39;][x] = remove_stopwords(chicago_df_text[&#39;tokenized_tw&#39;][x]) . nRecords = dallas_df_text[&#39;tokenized_tw&#39;].count() # nRecords = 3 # if the tokenized column is string change it to list for x in range(nRecords): if (isinstance(dallas_df_text[&#39;tokenized_tw&#39;][x],str)): # tokenListHere = toronto_df_text[&#39;tokenized_tw&#39;][0].split(&quot;,&quot;) dallas_df_text[&#39;tokenized_tw&#39;][x] = remove_stopwords(dallas_df_text[&#39;tokenized_tw&#39;][x]) . nRecords = houston_df_text[&#39;tokenized_tw&#39;].count() # nRecords = 3 # if the tokenized column is string change it to list for x in range(nRecords): if (isinstance(houston_df_text[&#39;tokenized_tw&#39;][x],str)): # tokenListHere = toronto_df_text[&#39;tokenized_tw&#39;][0].split(&quot;,&quot;) houston_df_text[&#39;tokenized_tw&#39;][x] = remove_stopwords(houston_df_text[&#39;tokenized_tw&#39;][x]) . nRecords = losangeles_df_text[&#39;tokenized_tw&#39;].count() # nRecords = 3 # if the tokenized column is string change it to list for x in range(nRecords): if (isinstance(losangeles_df_text[&#39;tokenized_tw&#39;][x],str)): # tokenListHere = toronto_df_text[&#39;tokenized_tw&#39;][0].split(&quot;,&quot;) losangeles_df_text[&#39;tokenized_tw&#39;][x] = remove_stopwords(losangeles_df_text[&#39;tokenized_tw&#39;][x]) . nRecords = newyork_df_text[&#39;tokenized_tw&#39;].count() # nRecords = 3 # if the tokenized column is string change it to list for x in range(nRecords): if (isinstance(newyork_df_text[&#39;tokenized_tw&#39;][x],str)): # tokenListHere = toronto_df_text[&#39;tokenized_tw&#39;][0].split(&quot;,&quot;) newyork_df_text[&#39;tokenized_tw&#39;][x] = remove_stopwords(newyork_df_text[&#39;tokenized_tw&#39;][x]) . nRecords = philadelphia_df_text[&#39;tokenized_tw&#39;].count() # nRecords = 3 # if the tokenized column is string change it to list for x in range(nRecords): if (isinstance(philadelphia_df_text[&#39;tokenized_tw&#39;][x],str)): # tokenListHere = toronto_df_text[&#39;tokenized_tw&#39;][0].split(&quot;,&quot;) philadelphia_df_text[&#39;tokenized_tw&#39;][x] = remove_stopwords(philadelphia_df_text[&#39;tokenized_tw&#39;][x]) . nRecords = phoenix_df_text[&#39;tokenized_tw&#39;].count() # nRecords = 3 # if the tokenized column is string change it to list for x in range(nRecords): if (isinstance(phoenix_df_text[&#39;tokenized_tw&#39;][x],str)): # tokenListHere = toronto_df_text[&#39;tokenized_tw&#39;][0].split(&quot;,&quot;) phoenix_df_text[&#39;tokenized_tw&#39;][x] = remove_stopwords(phoenix_df_text[&#39;tokenized_tw&#39;][x]) . nRecords = sanantonio_df_text[&#39;tokenized_tw&#39;].count() # nRecords = 3 # if the tokenized column is string change it to list for x in range(nRecords): if (isinstance(sanantonio_df_text[&#39;tokenized_tw&#39;][x],str)): # tokenListHere = toronto_df_text[&#39;tokenized_tw&#39;][0].split(&quot;,&quot;) sanantonio_df_text[&#39;tokenized_tw&#39;][x] = remove_stopwords(sanantonio_df_text[&#39;tokenized_tw&#39;][x]) . nRecords = sandiego_df_text[&#39;tokenized_tw&#39;].count() # nRecords = 3 # if the tokenized column is string change it to list for x in range(nRecords): if (isinstance(sandiego_df_text[&#39;tokenized_tw&#39;][x],str)): # tokenListHere = toronto_df_text[&#39;tokenized_tw&#39;][0].split(&quot;,&quot;) sandiego_df_text[&#39;tokenized_tw&#39;][x] = remove_stopwords(sandiego_df_text[&#39;tokenized_tw&#39;][x]) . B.2 Apply Filters - stemming words . from nltk.stem.porter import PorterStemmer . def applyporter(TokenList): # create an object for stemming porterstemmer = PorterStemmer() # TokenList = ast.literal_eval(TokenList) if isinstance(TokenList, str): TokenList = TokenList.split(&quot;,&quot;) for i, key in enumerate(TokenList): TokenList [i] = porterstemmer.stem(key) return TokenList # tokens = word_tokenize(text) nRecords_toronto = toronto_df_text[&#39;tokenized_tw&#39;].count() # apply porterstemmer on tokens for x_toronto in range(nRecords_toronto-1): toronto_df_text[&#39;tokenized_tw&#39;][x_toronto] = applyporter(toronto_df_text[&#39;tokenized_tw&#39;][x_toronto]) nRecords_chicago = chicago_df_text[&#39;tokenized_tw&#39;].count() # apply porterstemmer on tokens for x_chicago in range(nRecords_chicago-1): chicago_df_text[&#39;tokenized_tw&#39;][x_chicago] = applyporter(chicago_df_text[&#39;tokenized_tw&#39;][x_chicago]) . nRecords_dallas = dallas_df_text[&#39;tokenized_tw&#39;].count() # apply porterstemmer on tokens for x_dallas in range(nRecords_dallas-1): dallas_df_text[&#39;tokenized_tw&#39;][x_dallas] = applyporter(dallas_df_text[&#39;tokenized_tw&#39;][x_dallas]) nRecords_houston = houston_df_text[&#39;tokenized_tw&#39;].count() # apply porterstemmer on tokens for x_houston in range(nRecords_houston-1): houston_df_text[&#39;tokenized_tw&#39;][x_houston] = applyporter(houston_df_text[&#39;tokenized_tw&#39;][x_houston]) nRecords_losangeles = losangeles_df_text[&#39;tokenized_tw&#39;].count() # apply porterstemmer on tokens for x_losangeles in range(nRecords_losangeles-1): losangeles_df_text[&#39;tokenized_tw&#39;][x_losangeles] = applyporter(losangeles_df_text[&#39;tokenized_tw&#39;][x_losangeles]) nRecords_newyork = newyork_df_text[&#39;tokenized_tw&#39;].count() # apply porterstemmer on tokens for x_newyork in range(nRecords_newyork-1): newyork_df_text[&#39;tokenized_tw&#39;][x_newyork] = applyporter(newyork_df_text[&#39;tokenized_tw&#39;][x_newyork]) nRecords_philadelphia = philadelphia_df_text[&#39;tokenized_tw&#39;].count() # apply porterstemmer on tokens for x_philadelphia in range(nRecords_philadelphia-1): philadelphia_df_text[&#39;tokenized_tw&#39;][x_philadelphia] = applyporter(philadelphia_df_text[&#39;tokenized_tw&#39;][x_philadelphia]) nRecords_phoenix = phoenix_df_text[&#39;tokenized_tw&#39;].count() # apply porterstemmer on tokens for x_phoenix in range(nRecords_phoenix-1): phoenix_df_text[&#39;tokenized_tw&#39;][x_phoenix] = applyporter(phoenix_df_text[&#39;tokenized_tw&#39;][x_phoenix]) nRecords_sanantonio = sanantonio_df_text[&#39;tokenized_tw&#39;].count() # apply porterstemmer on tokens for x_sanantonio in range(nRecords_sanantonio-1): sanantonio_df_text[&#39;tokenized_tw&#39;][x_sanantonio] = applyporter(sanantonio_df_text[&#39;tokenized_tw&#39;][x_sanantonio]) nRecords_sandiego = sandiego_df_text[&#39;tokenized_tw&#39;].count() # apply porterstemmer on tokens for x_sandiego in range(nRecords_sandiego-1): sandiego_df_text[&#39;tokenized_tw&#39;][x_sandiego] = applyporter(sandiego_df_text[&#39;tokenized_tw&#39;][x_sandiego]) . Return the cleaned data into a sentence format . for i in range(nRecords_toronto): if isinstance(toronto_df_text[&#39;tokenized_tw&#39;][i],list): toronto_df_text[&#39;tokenized_tw&#39;][i] = &quot; &quot;.join(toronto_df_text[&#39;tokenized_tw&#39;][i]) . for i in range(nRecords_chicago): if isinstance(chicago_df_text[&#39;tokenized_tw&#39;][i],list): chicago_df_text[&#39;tokenized_tw&#39;][i] = &quot; &quot;.join(chicago_df_text[&#39;tokenized_tw&#39;][i]) . for i in range(nRecords_dallas): if isinstance(dallas_df_text[&#39;tokenized_tw&#39;][i],list): dallas_df_text[&#39;tokenized_tw&#39;][i] = &quot; &quot;.join(dallas_df_text[&#39;tokenized_tw&#39;][i]) ## combine word into a single sentence for i in range(nRecords_houston): if isinstance(houston_df_text[&#39;tokenized_tw&#39;][i],list): houston_df_text[&#39;tokenized_tw&#39;][i] = &quot; &quot;.join(houston_df_text[&#39;tokenized_tw&#39;][i]) ## combine word into a single sentence for i in range(nRecords_losangeles): if isinstance(losangeles_df_text[&#39;tokenized_tw&#39;][i],list): losangeles_df_text[&#39;tokenized_tw&#39;][i] = &quot; &quot;.join(losangeles_df_text[&#39;tokenized_tw&#39;][i]) ## combine word into a single sentence for i in range(nRecords_newyork): if isinstance(newyork_df_text[&#39;tokenized_tw&#39;][i],list): newyork_df_text[&#39;tokenized_tw&#39;][i] = &quot; &quot;.join(newyork_df_text[&#39;tokenized_tw&#39;][i]) ## combine word into a single sentence for i in range(nRecords_philadelphia): if isinstance(philadelphia_df_text[&#39;tokenized_tw&#39;][i],list): philadelphia_df_text[&#39;tokenized_tw&#39;][i] = &quot; &quot;.join(philadelphia_df_text[&#39;tokenized_tw&#39;][i]) ## combine word into a single sentence for i in range(nRecords_phoenix): if isinstance(phoenix_df_text[&#39;tokenized_tw&#39;][i],list): phoenix_df_text[&#39;tokenized_tw&#39;][i] = &quot; &quot;.join(phoenix_df_text[&#39;tokenized_tw&#39;][i]) ## combine word into a single sentence for i in range(nRecords_sanantonio): if isinstance(sanantonio_df_text[&#39;tokenized_tw&#39;][i],list): sanantonio_df_text[&#39;tokenized_tw&#39;][i] = &quot; &quot;.join(sanantonio_df_text[&#39;tokenized_tw&#39;][i]) ## combine word into a single sentence for i in range(nRecords_sandiego): if isinstance(sandiego_df_text[&#39;tokenized_tw&#39;][i],list): sandiego_df_text[&#39;tokenized_tw&#39;][i] = &quot; &quot;.join(sandiego_df_text[&#39;tokenized_tw&#39;][i]) . toronto_df_text[&#39;text&#39;] = toronto_df_text[&#39;text&#39;].apply(cleanTweets) toronto_df_text[&#39;text&#39;] = toronto_df_text[&#39;text&#39;].apply(cleanEmoji) . Create a WordCloud from cleaned tweets . . # Toronto - revisit wordcloud after cleaning allWords_toronto = &#39; &#39;.join(twts_toronto for twts_toronto in toronto_df_text[&#39;tokenized_tw&#39;]) wordCloud_toronto = WordCloud(width=800, height=500, random_state=21,max_font_size=119).generate(allWords_toronto) # Toronto - size the plt object plt.figure(figsize=(19,8)) plt.title(&#39;Toronto&#39;) plt.imshow(wordCloud_toronto, interpolation= &#39;bilinear&#39;) plt.axis(&#39;off&#39;) plt.show() # Chicago - revisit wordcloud after cleaning allWords_chicago = &#39; &#39;.join(twts_chicago for twts_chicago in chicago_df_text[&#39;tokenized_tw&#39;]) wordCloud_chicago = WordCloud(width=800, height=500, random_state=21,max_font_size=119).generate(allWords_chicago) # Toronto - size the plt object plt.figure(figsize=(19,8)) plt.title(&#39;Chicago&#39;) plt.imshow(wordCloud_chicago, interpolation= &#39;bilinear&#39;) plt.axis(&#39;off&#39;) plt.show() . Consider Location and Time in analysis . toronto_df_text.groupby([&#39;place&#39;]).count chicago_df_text.groupby([&#39;place&#39;]).count dallas_df_text.groupby([&#39;place&#39;]).count houston_df_text.groupby([&#39;place&#39;]).count losangeles_df_text.groupby([&#39;place&#39;]).count newyork_df_text.groupby([&#39;place&#39;]).count philadelphia_df_text.groupby([&#39;place&#39;]).count phoenix_df_text.groupby([&#39;place&#39;]).count sanantonio_df_text.groupby([&#39;place&#39;]).count sandiego_df_text.groupby([&#39;place&#39;]).count . &lt;bound method DataFrameGroupBy.count of &lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f5b6a74e190&gt;&gt; . from datetime import datetime from dateutil.parser import parse . toronto_df_text[&#39;date&#39;] = toronto_df_text[&#39;created_at&#39;] # Add a new column for date and convert created_at field for x in range(nRecords): # return date as Series toronto_df_text[&#39;date&#39;][x] = datetime.strptime(toronto_df_text[&#39;created_at&#39;][x],&#39;%a %b %d %H:%M:%S +0000 %Y&#39;) # Extract month from the data variable, Add a new column toronto_df_text[&#39;month&#39;] = pd.DatetimeIndex(toronto_df_text[&#39;date&#39;]).month . chicago_df_text[&#39;date&#39;] = chicago_df_text[&#39;created_at&#39;] # Add a new column for date and convert created_at field for x in range(nRecords): # return date as Series chicago_df_text[&#39;date&#39;][x] = datetime.strptime(chicago_df_text[&#39;created_at&#39;][x],&#39;%a %b %d %H:%M:%S +0000 %Y&#39;) # Extract month from the data variable, Add a new column chicago_df_text[&#39;month&#39;] = pd.DatetimeIndex(chicago_df_text[&#39;date&#39;]).month . dallas_df_text[&#39;date&#39;] = dallas_df_text[&#39;created_at&#39;] # Add a new column for date and convert created_at field for x in range(nRecords): # return date as Series dallas_df_text[&#39;date&#39;][x] = datetime.strptime(dallas_df_text[&#39;created_at&#39;][x],&#39;%a %b %d %H:%M:%S +0000 %Y&#39;) # Extract month from the data variable, Add a new column dallas_df_text[&#39;month&#39;] = pd.DatetimeIndex(dallas_df_text[&#39;date&#39;]).month ##create a new column for date only houston_df_text[&#39;date&#39;] = houston_df_text[&#39;created_at&#39;] # Add a new column for date and convert created_at field for x in range(nRecords): # return date as Series houston_df_text[&#39;date&#39;][x] = datetime.strptime(houston_df_text[&#39;created_at&#39;][x],&#39;%a %b %d %H:%M:%S +0000 %Y&#39;) # Extract month from the data variable, Add a new column houston_df_text[&#39;month&#39;] = pd.DatetimeIndex(houston_df_text[&#39;date&#39;]).month ##create a new column for date only losangeles_df_text[&#39;date&#39;] = losangeles_df_text[&#39;created_at&#39;] # Add a new column for date and convert created_at field for x in range(nRecords): # return date as Series losangeles_df_text[&#39;date&#39;][x] = datetime.strptime(losangeles_df_text[&#39;created_at&#39;][x],&#39;%a %b %d %H:%M:%S +0000 %Y&#39;) # Extract month from the data variable, Add a new column losangeles_df_text[&#39;month&#39;] = pd.DatetimeIndex(losangeles_df_text[&#39;date&#39;]).month ##create a new column for date only newyork_df_text[&#39;date&#39;] = newyork_df_text[&#39;created_at&#39;] # Add a new column for date and convert created_at field for x in range(nRecords): # return date as Series newyork_df_text[&#39;date&#39;][x] = datetime.strptime(newyork_df_text[&#39;created_at&#39;][x],&#39;%a %b %d %H:%M:%S +0000 %Y&#39;) # Extract month from the data variable, Add a new column newyork_df_text[&#39;month&#39;] = pd.DatetimeIndex(newyork_df_text[&#39;date&#39;]).month ##create a new column for date only philadelphia_df_text[&#39;date&#39;] = philadelphia_df_text[&#39;created_at&#39;] # Add a new column for date and convert created_at field for x in range(nRecords): # return date as Series philadelphia_df_text[&#39;date&#39;][x] = datetime.strptime(philadelphia_df_text[&#39;created_at&#39;][x],&#39;%a %b %d %H:%M:%S +0000 %Y&#39;) # Extract month from the data variable, Add a new column philadelphia_df_text[&#39;month&#39;] = pd.DatetimeIndex(philadelphia_df_text[&#39;date&#39;]).month . sandiego_df_text[&#39;date&#39;] = sandiego_df_text[&#39;created_at&#39;] # Add a new column for date and convert created_at field for x in range(nRecords): # return date as Series sandiego_df_text[&#39;date&#39;][x] = datetime.strptime(sandiego_df_text[&#39;created_at&#39;][x],&#39;%a %b %d %H:%M:%S +0000 %Y&#39;) # Extract month from the data variable, Add a new column sandiego_df_text[&#39;month&#39;] = pd.DatetimeIndex(sandiego_df_text[&#39;date&#39;]).month . Data preparation for further visualizations . # placesRecords = toronto_df_text.groupby([&#39;place&#39;]) # get the first item in each place group # placesRecords.first() # 1 Toronto # remove ontario from place column lista = toronto_df_text.place.str.split(&quot;,&quot;,expand=True) toronto_df_text[&#39;place&#39;] = lista [0] # convert the polarity and subjectivity values to numbers toronto_df_text[&quot;polarity&quot;] = pd.to_numeric(toronto_df_text[&#39;polarity&#39;]) toronto_df_text[&quot;subjectivity&quot;] = pd.to_numeric(toronto_df_text[&#39;subjectivity&#39;]) # create new dataframes for further visualizations # get number of tweets from each place df_region_sentiment_toronto = toronto_df_text.groupby(&#39;place&#39;, as_index=False)[&#39;polarity&#39;].mean() # get the polarities and months information df_time_sentiment_toronto = toronto_df_text.groupby(&#39;month&#39;, as_index=False)[&#39;polarity&#39;].mean() . lista = chicago_df_text.place.str.split(&quot;,&quot;,expand=True) chicago_df_text[&#39;place&#39;] = lista [0] # convert the polarity and subjectivity values to numbers chicago_df_text[&quot;polarity&quot;] = pd.to_numeric(chicago_df_text[&#39;polarity&#39;]) chicago_df_text[&quot;subjectivity&quot;] = pd.to_numeric(chicago_df_text[&#39;subjectivity&#39;]) # create new dataframes for further visualizations # get number of tweets from each place df_region_sentiment_chicago = chicago_df_text.groupby(&#39;place&#39;, as_index=False)[&#39;polarity&#39;].mean() # get the polarities and months information df_time_sentiment_chicago = chicago_df_text.groupby(&#39;month&#39;, as_index=False)[&#39;polarity&#39;].mean() . lista = dallas_df_text.place.str.split(&quot;,&quot;,expand=True) dallas_df_text[&#39;place&#39;] = lista [0] # convert the polarity and subjectivity values to numbers dallas_df_text[&quot;polarity&quot;] = pd.to_numeric(dallas_df_text[&#39;polarity&#39;]) dallas_df_text[&quot;subjectivity&quot;] = pd.to_numeric(dallas_df_text[&#39;subjectivity&#39;]) # create new dataframes for further visualizations # get number of tweets from each place df_region_sentiment_dallas = dallas_df_text.groupby(&#39;place&#39;, as_index=False)[&#39;polarity&#39;].mean() # get the polarities and months information df_time_sentiment_dallas = dallas_df_text.groupby(&#39;month&#39;, as_index=False)[&#39;polarity&#39;].mean() . lista = houston_df_text.place.str.split(&quot;,&quot;,expand=True) houston_df_text[&#39;place&#39;] = lista [0] # convert the polarity and subjectivity values to numbers houston_df_text[&quot;polarity&quot;] = pd.to_numeric(houston_df_text[&#39;polarity&#39;]) houston_df_text[&quot;subjectivity&quot;] = pd.to_numeric(houston_df_text[&#39;subjectivity&#39;]) # create new dataframes for further visualizations # get number of tweets from each place df_region_sentiment_houston = houston_df_text.groupby(&#39;place&#39;, as_index=False)[&#39;polarity&#39;].mean() # get the polarities and months information df_time_sentiment_houston = houston_df_text.groupby(&#39;month&#39;, as_index=False)[&#39;polarity&#39;].mean() . lista = losangeles_df_text.place.str.split(&quot;,&quot;,expand=True) losangeles_df_text[&#39;place&#39;] = lista [0] # convert the polarity and subjectivity values to numbers losangeles_df_text[&quot;polarity&quot;] = pd.to_numeric(losangeles_df_text[&#39;polarity&#39;]) losangeles_df_text[&quot;subjectivity&quot;] = pd.to_numeric(losangeles_df_text[&#39;subjectivity&#39;]) # create new dataframes for further visualizations # get number of tweets from each place df_region_sentiment_losangeles = losangeles_df_text.groupby(&#39;place&#39;, as_index=False)[&#39;polarity&#39;].mean() # get the polarities and months information df_time_sentiment_losangeles = losangeles_df_text.groupby(&#39;month&#39;, as_index=False)[&#39;polarity&#39;].mean() . lista = newyork_df_text.place.str.split(&quot;,&quot;,expand=True) newyork_df_text[&#39;place&#39;] = lista [0] # convert the polarity and subjectivity values to numbers newyork_df_text[&quot;polarity&quot;] = pd.to_numeric(newyork_df_text[&#39;polarity&#39;]) newyork_df_text[&quot;subjectivity&quot;] = pd.to_numeric(newyork_df_text[&#39;subjectivity&#39;]) # create new dataframes for further visualizations # get number of tweets from each place df_region_sentiment_newyork = newyork_df_text.groupby(&#39;place&#39;, as_index=False)[&#39;polarity&#39;].mean() # get the polarities and months information df_time_sentiment_newyork = newyork_df_text.groupby(&#39;month&#39;, as_index=False)[&#39;polarity&#39;].mean() . lista = philadelphia_df_text.place.str.split(&quot;,&quot;,expand=True) philadelphia_df_text[&#39;place&#39;] = lista [0] # convert the polarity and subjectivity values to numbers philadelphia_df_text[&quot;polarity&quot;] = pd.to_numeric(philadelphia_df_text[&#39;polarity&#39;]) philadelphia_df_text[&quot;subjectivity&quot;] = pd.to_numeric(philadelphia_df_text[&#39;subjectivity&#39;]) # create new dataframes for further visualizations # get number of tweets from each place df_region_sentiment_philadelphia = philadelphia_df_text.groupby(&#39;place&#39;, as_index=False)[&#39;polarity&#39;].mean() # get the polarities and months information df_time_sentiment_philadelphia = philadelphia_df_text.groupby(&#39;month&#39;, as_index=False)[&#39;polarity&#39;].mean() . lista = sandiego_df_text.place.str.split(&quot;,&quot;,expand=True) sandiego_df_text[&#39;place&#39;] = lista [0] # convert the polarity and subjectivity values to numbers sandiego_df_text[&quot;polarity&quot;] = pd.to_numeric(sandiego_df_text[&#39;polarity&#39;]) sandiego_df_text[&quot;subjectivity&quot;] = pd.to_numeric(sandiego_df_text[&#39;subjectivity&#39;]) # create new dataframes for further visualizations # get number of tweets from each place df_region_sentiment_sandiego = sandiego_df_text.groupby(&#39;place&#39;, as_index=False)[&#39;polarity&#39;].mean() # get the polarities and months information df_time_sentiment_sandiego = sandiego_df_text.groupby(&#39;month&#39;, as_index=False)[&#39;polarity&#39;].mean() . #df_region_sentiment_toronto[&quot;polarity&quot;] = pd.to_numeric(df_region_sentiment_toronto[&#39;polarity&#39;]) df_region_sentiment_toronto.sort_values(by=&#39;polarity&#39;).plot.barh(x=&#39;place&#39;, y=&#39;polarity&#39;, rot=0, figsize=(15, 10)) df_region_sentiment_chicago.sort_values(by=&#39;polarity&#39;).plot.barh(x=&#39;place&#39;, y=&#39;polarity&#39;, rot=0, figsize=(15, 50)) . &lt;AxesSubplot:ylabel=&#39;place&#39;&gt; . df_time_sentiment.head(5) . month polarity . 0 3 | 0.096624 | . 1 4 | 0.136342 | . 2 5 | 0.127804 | . 3 6 | 0.115753 | . 4 7 | 0.126581 | . df_time_sentiment_toronto.sort_values(by=&#39;month&#39;).plot.line(x=&#39;month&#39;, y=&#39;polarity&#39;, rot=0, figsize=(10, 7), color=&quot;blue&quot;,title=&#39;Toronto&#39;) df_time_sentiment_chicago.sort_values(by=&#39;month&#39;).plot.line(x=&#39;month&#39;, y=&#39;polarity&#39;, rot=0, figsize=(10, 7), color=&quot;blue&quot;,title=&#39;Chicago&#39;) df_time_sentiment_dallas.sort_values(by=&#39;month&#39;).plot.line(x=&#39;month&#39;, y=&#39;polarity&#39;, rot=0, figsize=(10, 7), color=&quot;blue&quot;,title=&#39;Dallas&#39;) df_time_sentiment_houston.sort_values(by=&#39;month&#39;).plot.line(x=&#39;month&#39;, y=&#39;polarity&#39;, rot=0, figsize=(10, 7), color=&quot;blue&quot;,title=&#39;Houston&#39;) df_time_sentiment_losangeles.sort_values(by=&#39;month&#39;).plot.line(x=&#39;month&#39;, y=&#39;polarity&#39;, rot=0, figsize=(10, 7), color=&quot;blue&quot;,title=&#39;LosAngeles&#39;) df_time_sentiment_newyork.sort_values(by=&#39;month&#39;).plot.line(x=&#39;month&#39;, y=&#39;polarity&#39;, rot=0, figsize=(10, 7), color=&quot;blue&quot;,title=&#39;NewYork&#39;) df_time_sentiment_philadelphia.sort_values(by=&#39;month&#39;).plot.line(x=&#39;month&#39;, y=&#39;polarity&#39;, rot=0, figsize=(10, 7), color=&quot;blue&quot;,title=&#39;Philadelphia&#39;) df_time_sentiment_sandiego.sort_values(by=&#39;month&#39;).plot.line(x=&#39;month&#39;, y=&#39;polarity&#39;, rot=0, figsize=(10, 7), color=&quot;blue&quot;,title=&#39;SanDiego&#39;) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;SanDiego&#39;}, xlabel=&#39;month&#39;&gt; .",
            "url": "https://sra00.github.io/notes1/tweetanalysis/2022/06/04/text-analysis-1.html",
            "relUrl": "/tweetanalysis/2022/06/04/text-analysis-1.html",
            "date": " • Jun 4, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Emotions & Images in Tweets - Part2",
            "content": "Approach . Creating a Custom Convolutional Network . Beyond the Text . Beside the text, Twitter messages often include images, emojis or other special characters. One approach to understand the overall sentiments from tweets is to work with each layer of information separately and then combining the analysis to reach a result that reflects all layers of the information. This section will look at images separate from the text. Unpacking a way of analyzing images. . How to understand images? . We often understand a complex set of meaning from an image through a quick glance but what does it take for an automated system to understand the interplay of meaning in an image. There are multiple approaches in understanding an image. For instance, an image could be segregating into objects through image segmentation and using further meaning driven analysis a meaning of the image can be formed. But a holistic approach could prove beneficial too. Considering the multitude and complexities that are involved in making sense of images. The first approach here is use a pre-trained resnet model to identify emotions through images. The idea is that we could train a Machine Learning model with a set of images. After the initial training stage, the model should be able to identify the meaning of any new image that we get from tweets for instance. . Project Goals . In this project, I’m looking to identify the overall reaction around a certain topic. One approach would be to identify tweets based on their underlying sentiment, such as anger, joy, sadness, etc. The first step is to find a dataset which includes images with their corresponding labels. This dataset can be used to train the initial model. Looking for such a dataset, I found [Emotion6] dataset from [Cornel University AMP lab] (http://chenlab.ece.cornell.edu/downloads.html). Emotion6 is a collection of 1980 images and is divided into six categories of anger, disgust, fear, joy sadness, neutral . . Citations . [1] Kuan-Chuan Peng, Amir Sadovnik, Andrew Gallagher, and Tsuhan Chen. &quot;A Mixed Bag of Emotions: Model, Predict, and Transfer Emotion Distributions.&quot;, Computer Vision and Pattern Recognition (CVPR), 2015. . Step 1- loading data into numpy array &amp; Normalize it divide into test and training datasets . %matplotlib inline . import torch import torchvision import os import numpy as np import glob import torch.nn as nn from torchvision.transforms import transforms from torch.utils.data import DataLoader from torch.optim import Adam from torch.autograd import Variable import torchvision import pathlib import torch.nn.functional as F import matplotlib.pyplot as plt from torchvision import models . . device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) . device . device(type=&#39;cuda&#39;, index=0) . # Image.open(&#39;/Data/Emotions6/Images&#39;).convert(&#39;RGB&#39;) . transformer = transforms.Compose([ transforms.Resize((150,150)), transforms.RandomHorizontalFlip(), transforms.ToTensor(), #0-255 to 0-1, num py to tensors transforms.Normalize([0.5,0.5,0.5], # 0-1 to 1- to 1 [0.5,0.5,0.5]) ]) . test_path = &#39;Data/Emotion6/images/emot_test&#39; train_path = &#39;Data/Emotion6/images/emot_train&#39; batch_size= 10 #DataLoader train_loader = DataLoader( torchvision.datasets.ImageFolder(train_path,transform=transformer), batch_size= 10, shuffle = True ) test_loader = DataLoader( torchvision.datasets.ImageFolder(test_path,transform=transformer), batch_size= 10, shuffle = True ) . dataiter = iter(train_loader) images, labels = dataiter.next() . def imshow(img): img = img / 2 + 0.5 # unnormalize npimg = img.numpy() plt.imshow(np.transpose(npimg, (1, 2, 0))) plt.show() . root = pathlib.Path(train_path) classes = sorted([j.name.split(&#39;/&#39;)[-1] for j in root.iterdir()]) . print(classes) . [&#39;anger&#39;, &#39;disgust&#39;, &#39;fear&#39;, &#39;joy&#39;, &#39;sadness&#39;, &#39;surprise&#39;] . def imshow(img): img = img / 2 + 0.5 # unnormalize plt.imshow(np.transpose(img, (1, 2, 0))) # convert from Tensor image # obtain one batch of training images dataiter = iter(train_loader) images, labels = dataiter.next() images = images.numpy() # convert images to numpy for display # plot the images in the batch, along with the corresponding labels fig = plt.figure(figsize=(10, 4)) # display 10 images for idx in np.arange(10): ax = fig.add_subplot(2, 10/2, idx+1, xticks=[], yticks=[]) imshow(images[idx]) ax.set_title(classes[labels[idx]]) . /tmp/ipykernel_3496/1155777525.py:15: MatplotlibDeprecationWarning: Passing non-integers as three-element position specification is deprecated since 3.3 and will be removed two minor releases later. ax = fig.add_subplot(2, 10/2, idx+1, xticks=[], yticks=[]) . class ConvNet(nn.Module): def __init__(self,num_classes=6): super(ConvNet,self).__init__() #Output size after convolution filter #((w-f+2P)/s) +1 #Input shape= (256,3,150,150) self.conv1=nn.Conv2d(in_channels=3,out_channels=12,kernel_size=3,stride=1,padding=1) self.bn1=nn.BatchNorm2d(num_features=12) self.relu1=nn.ReLU() self.pool=nn.MaxPool2d(kernel_size=2) self.conv2=nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1) self.relu2=nn.ReLU() self.conv3=nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1) self.bn3=nn.BatchNorm2d(num_features=32) self.relu3=nn.ReLU() # self.conv4=nn.Conv2d(in_channels=32,out_channels=46,kernel_size=3,stride=1,padding=1) # self.bn4=nn.BatchNorm2d(num_features=46) # self.relu4=nn.ReLU() self.fc=nn.Linear(in_features=75 * 75 * 32 ,out_features=num_classes) #Feed forwad function def forward(self,input): output=self.conv1(input) output=self.bn1(output) output=self.relu1(output) output=self.pool(output) output=self.conv2(output) output=self.relu2(output) output=self.conv3(output) output=self.bn3(output) output=self.relu3(output) #Above output will be in matrix form, with shape (256,32,75,75) output=output.view(-1,32*75*75) output=self.fc(output) return output . model = ConvNet(num_classes=6).to(device) # model = Net() # model = models.vgg16(pretrained=True) # model = models.vgg16(pretrained=True) . optimizer=Adam(model.parameters(),lr=0.001,weight_decay=0.0001) loss_function=nn.CrossEntropyLoss() . train_count=len(glob.glob(train_path+&#39;/**/*.jpg&#39;)) test_count=len(glob.glob(test_path+&#39;/**/*.jpg&#39;)) . print(train_count,test_count) . 1584 396 . num_epochs=11 . best_accuracy=0.0 for epoch in range(num_epochs): #1-Evaluation and training on training dataset model.train() train_accuracy=0.0 train_loss=0.0 #training loader loop for i, (images,labels) in enumerate(train_loader): if torch.cuda.is_available(): images=Variable(images.cuda()) labels=Variable(labels.cuda()) optimizer.zero_grad() outputs=model(images) loss=loss_function(outputs,labels) loss.backward() optimizer.step() train_loss+= loss.cpu().data*images.size(0) _,prediction=torch.max(outputs.data,1) train_accuracy+=int(torch.sum(prediction==labels.data)) train_accuracy=train_accuracy/train_count train_loss=train_loss/train_count #2-Evaluation on testing dataset model.eval() test_accuracy=0.0 for i, (images,labels) in enumerate(test_loader): if torch.cuda.is_available(): images=Variable(images.cuda()) labels=Variable(labels.cuda()) outputs=model(images) _,prediction=torch.max(outputs.data,1) test_accuracy+=int(torch.sum(prediction==labels.data)) test_accuracy=test_accuracy/test_count print(&#39;Epoch: &#39;+str(epoch)+&#39; Train Loss: &#39;+str(train_loss)+&#39; Train Accuracy: &#39;+str(train_accuracy)+&#39; Test Accuracy: &#39;+str(test_accuracy)) #Save the best model if test_accuracy&gt;best_accuracy: torch.save(model.state_dict(),&#39;best_checkpoint_CNN1.model&#39;) best_accuracy=test_accuracy . Epoch: 0 Train Loss: tensor(17.3005) Train Accuracy: 0.20012626262626262 Test Accuracy: 0.23484848484848486 Epoch: 1 Train Loss: tensor(5.1706) Train Accuracy: 0.39330808080808083 Test Accuracy: 0.20707070707070707 Epoch: 2 Train Loss: tensor(1.8187) Train Accuracy: 0.5271464646464646 Test Accuracy: 0.22474747474747475 Epoch: 3 Train Loss: tensor(0.9630) Train Accuracy: 0.6830808080808081 Test Accuracy: 0.2601010101010101 Epoch: 4 Train Loss: tensor(0.5627) Train Accuracy: 0.8244949494949495 Test Accuracy: 0.2196969696969697 Epoch: 5 Train Loss: tensor(0.4051) Train Accuracy: 0.8819444444444444 Test Accuracy: 0.20959595959595959 Epoch: 6 Train Loss: tensor(0.3027) Train Accuracy: 0.9084595959595959 Test Accuracy: 0.2297979797979798 Epoch: 7 Train Loss: tensor(0.2230) Train Accuracy: 0.9311868686868687 Test Accuracy: 0.255050505050505 Epoch: 8 Train Loss: tensor(0.1864) Train Accuracy: 0.9457070707070707 Test Accuracy: 0.24494949494949494 Epoch: 9 Train Loss: tensor(0.1427) Train Accuracy: 0.9595959595959596 Test Accuracy: 0.22474747474747475 Epoch: 10 Train Loss: tensor(0.1100) Train Accuracy: 0.9678030303030303 Test Accuracy: 0.24494949494949494 .",
            "url": "https://sra00.github.io/notes1/jupyter/2022/06/04/image-2.html",
            "relUrl": "/jupyter/2022/06/04/image-2.html",
            "date": " • Jun 4, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Tweet Image Emotion detection - Part 2",
            "content": "About . Prediction using best_checkpoint_CNN1 . Working with pre-trained models: Resnet18, Resnet50 . Beyond the Text . Beside the text, Twitter messages often include images, emojis or other special characters. One approach to understand the overall sentiments from tweets is to work with each layer of information separately and then combining the analysis to reach a result that reflects all layers of the information. This section will look at images separate from the text. Unpacking a way of analyzing images. . How to understand images? . We often understand a complex set of meaning from an image through a quick glance but what does it take for an automated system to understand the interplay of meaning in an image. There are multiple approaches in understanding an image. For instance, an image could be segregating into objects through image segmentation and using further meaning driven analysis a meaning of the image can be formed. But a holistic approach could prove beneficial too. Considering the multitude and complexities that are involved in making sense of images. The first approach here is use a pre-trained resnet model to identify emotions through images. The idea is that we could train a Machine Learning model with a set of images. After the initial training stage, the model should be able to identify the meaning of any new image that we get from tweets for instance. . Project Goals . In this project, I’m looking to identify the overall reaction around a certain topic. One approach would be to identify tweets based on their underlying sentiment, such as anger, joy, sadness, etc. The first step is to find a dataset which includes images with their corresponding labels. This dataset can be used to train the initial model. Looking for such a dataset, I found [Emotion6] dataset from [Cornel University AMP lab] (http://chenlab.ece.cornell.edu/downloads.html). Emotion6 is a collection of 1980 images and is divided into six categories of anger, disgust, fear, joy sadness, neutral . . Citations . [1] Kuan-Chuan Peng, Amir Sadovnik, Andrew Gallagher, and Tsuhan Chen. &quot;A Mixed Bag of Emotions: Model, Predict, and Transfer Emotion Distributions.&quot;, Computer Vision and Pattern Recognition (CVPR), 2015. . Training the model . import torch import torch.nn as nn from torchvision.transforms import transforms import numpy as np from torch.autograd import Variable from torchvision.models import squeezenet1_1 import torch.functional as F from io import open import os from PIL import Image import pathlib import glob from torchvision import models . train_path=&#39;Data/Emotion6/images/emot_train&#39; pred_path=&#39;Data/Emotion6/images/emot_pred&#39; . root=pathlib.Path(train_path) classes=sorted([j.name.split(&#39;/&#39;)[-1] for j in root.iterdir()]) . class ConvNet(nn.Module): def __init__(self,num_classes=6): super(ConvNet,self).__init__() #Output size after convolution filter #((w-f+2P)/s) +1 #Input shape= (256,3,150,150) self.conv1=nn.Conv2d(in_channels=3,out_channels=12,kernel_size=3,stride=1,padding=1) #Shape= (256,12,150,150) self.bn1=nn.BatchNorm2d(num_features=12) #Shape= (256,12,150,150) self.relu1=nn.ReLU() #Shape= (256,12,150,150) self.pool=nn.MaxPool2d(kernel_size=2) #Reduce the image size be factor 2 #Shape= (256,12,75,75) self.conv2=nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1) #Shape= (256,20,75,75) self.relu2=nn.ReLU() #Shape= (256,20,75,75) self.conv3=nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1) #Shape= (256,32,75,75) self.bn3=nn.BatchNorm2d(num_features=32) #Shape= (256,32,75,75) self.relu3=nn.ReLU() #Shape= (256,32,75,75) self.fc=nn.Linear(in_features=75 * 75 * 32,out_features=num_classes) #Feed forwad function def forward(self,input): output=self.conv1(input) output=self.bn1(output) output=self.relu1(output) output=self.pool(output) output=self.conv2(output) output=self.relu2(output) output=self.conv3(output) output=self.bn3(output) output=self.relu3(output) #Above output will be in matrix form, with shape (256,32,75,75) output=output.view(-1,32*75*75) output=self.fc(output) return output . class AlexNet(nn.Module): def __init__(self, num_classes): super(AlexNet, self).__init__() self.features = nn.Sequential( nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(64, 192, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(192, 384, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), ) self.avgpool = nn.AdaptiveAvgPool2d((6, 6)) self.classifier = nn.Sequential( nn.Dropout(0.5), nn.Linear(256 * 6 * 6, 4096), nn.ReLU(inplace=True), nn.Dropout(0.5), nn.Linear(4096, 4096), nn.ReLU(inplace=True), nn.Linear(4096, num_classes) ) def forward(self, x): x = self.features(x) x = self.avgpool(x) x = x.view(x.size(0), 256 * 6 * 6) logits = self.classifier(x) # probas = F.softmax(logits, dim=1) return logits . checkpoint=torch.load(&#39;best_checkpoint.model&#39;) # model = models.vgg16(pretrained=False) # model=ConvNet(num_classes=6) # model=AlexNet(num_classes=6) model = models.resnet18() model.load_state_dict(checkpoint) model.eval() . ResNet( (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (layer1): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer2): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer3): Sequential( (0): BasicBlock( (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer4): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (avgpool): AdaptiveAvgPool2d(output_size=(1, 1)) (fc): Linear(in_features=512, out_features=1000, bias=True) ) . transformer=transforms.Compose([ transforms.Resize((150,150)), transforms.ToTensor(), #0-255 to 0-1, numpy to tensors transforms.Normalize([0.5,0.5,0.5], # 0-1 to [-1,1] , formula (x-mean)/std [0.5,0.5,0.5]) ]) images_path=glob.glob(pred_path+&#39;/*.jpg&#39;) . def prediction(img_path,transformer): image=Image.open(img_path) image_tensor=transformer(image).float() image_tensor=image_tensor.unsqueeze_(0) if torch.cuda.is_available(): image_tensor.cuda() input=Variable(image_tensor) output=model(input) index=output.data.numpy().argmax() pred=classes[index] return pred . pred_dict={} for i in images_path: pred_dict[i[i.rfind(&#39;/&#39;)+1:]]=prediction(i,transformer) . pred_dict . {&#39;Image54.jpg&#39;: &#39;joy&#39;, &#39;Image454.jpg&#39;: &#39;joy&#39;, &#39;Image5657.jpg&#39;: &#39;joy&#39;, &#39;ksGhYGkj_bigger.jpg&#39;: &#39;joy&#39;, &#39;sgcP1BRg_bigger.jpg&#39;: &#39;sadness&#39;, &#39;94128202_2868821099834477_4813158761750645393_n.jpg&#39;: &#39;sadness&#39;, &#39;Image (11).jpg&#39;: &#39;joy&#39;, &#39;94103955_161645945308280_8771195847189162660_n.jpg&#39;: &#39;sadness&#39;, &#39;Image (1).jpg&#39;: &#39;surprise&#39;, &#39;Image56777.jpg&#39;: &#39;joy&#39;, &#39;Image567.jpg&#39;: &#39;surprise&#39;, &#39;Image (3)676.jpg&#39;: &#39;joy&#39;, &#39;Image757575.jpg&#39;: &#39;joy&#39;, &#39;Image444.jpg&#39;: &#39;joy&#39;, &#39;Image (10).jpg&#39;: &#39;disgust&#39;, &#39;E6rPXNjWUAIBjJa.jpg&#39;: &#39;disgust&#39;, &#39;_4.jpg&#39;: &#39;disgust&#39;, &#39;rHzX97pL_bigger.jpg&#39;: &#39;joy&#39;, &#39;92923611_1318724421851566_555606817225749807_n.jpg&#39;: &#39;joy&#39;, &#39;dcTC4Iqh_bigger.jpg&#39;: &#39;joy&#39;, &#39;Image5555.jpg&#39;: &#39;disgust&#39;, &#39;Icb_qncl_bigger.jpg&#39;: &#39;joy&#39;, &#39;Image8768.jpg&#39;: &#39;fear&#39;, &#39;Image5467.jpg&#39;: &#39;joy&#39;, &#39;_1.jpg&#39;: &#39;joy&#39;, &#39;Image (13).jpg&#39;: &#39;disgust&#39;, &#39;Image656565.jpg&#39;: &#39;disgust&#39;, &#39;Image (3).jpg&#39;: &#39;fear&#39;, &#39;Image54545.jpg&#39;: &#39;disgust&#39;, &#39;94063514_366264130974642_4624767354939302349_n.jpg&#39;: &#39;sadness&#39;, &#39;Image999.jpg&#39;: &#39;joy&#39;, &#39;lfASSkmiIqbwBELK.jpg&#39;: &#39;joy&#39;, &#39;Image23.jpg&#39;: &#39;fear&#39;, &#39;Image45388.jpg&#39;: &#39;sadness&#39;, &#39;Image887.jpg&#39;: &#39;sadness&#39;, &#39;Image531.jpg&#39;: &#39;joy&#39;, &#39;Image55.jpg&#39;: &#39;joy&#39;, &#39;Image (1)56756.jpg&#39;: &#39;joy&#39;, &#39;Image.jpg&#39;: &#39;fear&#39;, &#39;Image64.jpg&#39;: &#39;disgust&#39;, &#39;Image743.jpg&#39;: &#39;joy&#39;, &#39;w-TpNJ0OZZh_SLPf.jpg&#39;: &#39;joy&#39;, &#39;Image (12).jpg&#39;: &#39;fear&#39;, &#39;Image (2).jpg&#39;: &#39;fear&#39;, &#39;Image (1)5553.jpg&#39;: &#39;sadness&#39;, &#39;Image (1)5.jpg&#39;: &#39;disgust&#39;, &#39;Image566.jpg&#39;: &#39;joy&#39;, &#39;E7VO8l3X0AUWw-2.jpg&#39;: &#39;disgust&#39;, &#39;97155132_673810500118160_1507424695014653952_n.jpg&#39;: &#39;sadness&#39;, &#39;Image (15).jpg&#39;: &#39;disgust&#39;, &#39;Image5644.jpg&#39;: &#39;fear&#39;, &#39;Image657.jpg&#39;: &#39;sadness&#39;, &#39;Image987.jpg&#39;: &#39;joy&#39;, &#39;Image (3)546.jpg&#39;: &#39;joy&#39;, &#39;Image3333.jpg&#39;: &#39;sadness&#39;, &#39;Image (2)5.jpg&#39;: &#39;fear&#39;, &#39;rCWOzatw_bigger.jpg&#39;: &#39;fear&#39;, &#39;Image (9).jpg&#39;: &#39;surprise&#39;, &#39;Image21.jpg&#39;: &#39;joy&#39;, &#39;Image56755.jpg&#39;: &#39;fear&#39;, &#39;Image (1)666.jpg&#39;: &#39;joy&#39;, &#39;Image4566.jpg&#39;: &#39;joy&#39;, &#39;Image (5).jpg&#39;: &#39;disgust&#39;, &#39;Image (14).jpg&#39;: &#39;disgust&#39;, &#39;Image 45.jpg&#39;: &#39;joy&#39;, &#39;Image66.jpg&#39;: &#39;joy&#39;, &#39;JI-9Xzz9_bigger.jpg&#39;: &#39;joy&#39;, &#39;Image234.jpg&#39;: &#39;fear&#39;, &#39;Image435667.jpg&#39;: &#39;joy&#39;, &#39;Image (8).jpg&#39;: &#39;disgust&#39;, &#39;Image354.jpg&#39;: &#39;disgust&#39;, &#39;Image (3)6786.jpg&#39;: &#39;joy&#39;, &#39;Image453.jpg&#39;: &#39;fear&#39;, &#39;P70X22Gj_bigger.jpg&#39;: &#39;joy&#39;, &#39;Image2344.jpg&#39;: &#39;sadness&#39;, &#39;Image4576.jpg&#39;: &#39;joy&#39;, &#39;201301956_4484701894887235_1035576262456272086_n.jpg&#39;: &#39;fear&#39;, &#39;Image (4).jpg&#39;: &#39;fear&#39;, &#39;Image4444.jpg&#39;: &#39;joy&#39;, &#39;Image456.jpg&#39;: &#39;surprise&#39;, &#39;Image56.jpg&#39;: &#39;surprise&#39;, &#39;Image 44.jpg&#39;: &#39;sadness&#39;, &#39;Image (1)32.jpg&#39;: &#39;fear&#39;, &#39;Image54675.jpg&#39;: &#39;joy&#39;, &#39;Image (1)7567.jpg&#39;: &#39;disgust&#39;, &#39;Image63.jpg&#39;: &#39;surprise&#39;, &#39;Image (2)546.jpg&#39;: &#39;joy&#39;, &#39;Image (7).jpg&#39;: &#39;disgust&#39;, &#39;Image (1)456.jpg&#39;: &#39;joy&#39;, &#39;Image46.jpg&#39;: &#39;joy&#39;, &#39;_2.jpg&#39;: &#39;sadness&#39;, &#39;OM4piOux_bigger.jpg&#39;: &#39;joy&#39;, &#39;E0N7PTlx_bigger.jpg&#39;: &#39;joy&#39;, &#39;zl6Llik5_bigger.jpg&#39;: &#39;joy&#39;, &#39;93767281_223116045801198_8078775460886576050_n.jpg&#39;: &#39;fear&#39;, &#39;Image (2)5345.jpg&#39;: &#39;disgust&#39;, &#39;Image3555.jpg&#39;: &#39;joy&#39;, &#39;Image (6).jpg&#39;: &#39;surprise&#39;} . # def imshow(img): # img = img / 2 + 0.5 # unnormalize # plt.imshow(np.transpose(img, (1, 2, 0))) # convert from Tensor image # # obtain one batch of training images # dataiter = iter(train_loader) # images, labels = dataiter.next() # images = images.numpy() # convert images to numpy for display # # plot the images in the batch, along with the corresponding labels # fig = plt.figure(figsize=(10, 4)) # # display 20 images # for idx in np.arange(10): # ax = fig.add_subplot(2, 10/2, idx+1, xticks=[], yticks=[]) # imshow(images[idx]) # ax.set_title(classes[labels[idx]]) . from PIL import Image import matplotlib.pyplot as plt . img_n = 50 img_list = list(pred_dict.keys()) test_image = img_list[img_n] print(f&#39; The predicted label is: {pred_dict[test_image]}&#39;) Image.open(pred_path+&quot;/&quot;+test_image) . The predicted label is: fear . import random import cv2 from matplotlib import pyplot as plt . img_rand_imgs = random.choices(img_list,k=20) . # create figure fig = plt.figure(figsize=(20, 14)) # setting values to rows and column variables rows = 4 columns = 5 i=1 for x in img_rand_imgs: img_path = pred_path + &#39;/&#39;+ x img = cv2.imread(img_path) # Adds a subplot at the 1st position fig.add_subplot(rows, columns, i) # showing image # plt.imshow(img) plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)) plt.axis(&#39;off&#39;) plt.title( f&#39; &lt; {pred_dict[x]} &gt;&#39; ) i = i + 1 . type(pred_dict) . dict . import pandas as pd . pd_img_results = pd.DataFrame([pred_dict]) . df = pd.DataFrame(list(pred_dict.items()),columns = [&#39;filename&#39;,&#39;emotion&#39;]) grouped = df.groupby(&quot;emotion&quot;) . grouped.count() . filename . emotion . disgust 13 | . fear 5 | . joy 35 | . sadness 10 | . surprise 35 | .",
            "url": "https://sra00.github.io/notes1/jupyter/2021/11/08/pred2-Image-3.html",
            "relUrl": "/jupyter/2021/11/08/pred2-Image-3.html",
            "date": " • Nov 8, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://sra00.github.io/notes1/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is to showcase Twitter Analysis Project 1. . Includes Jupyter notebooks in addition to other files. &#8617; . |",
          "url": "https://sra00.github.io/notes1/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://sra00.github.io/notes1/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}